# @title
import json
import gc
import math
import time
import datetime
import os
import shutil
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import warnings
from collections import defaultdict

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Sampler
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import joblib
import matplotlib.pyplot as plt
import seaborn as sns


# ============================================================================
# 0. å…¨å±€å¸¸é‡ä¸ç¯å¢ƒè®¾ç½®
# ============================================================================

# æ•°æ®ç»´åº¦å¸¸é‡ï¼ˆé¿å…ç¡¬ç¼–ç ï¼Œä¾¿äºåç»­ä¿®æ”¹ï¼‰
DEFAULT_TOKEN_LEN = 11      # token åºåˆ—é•¿åº¦
DEFAULT_FEAT_DIM = 11       # æ¯ä¸ª token çš„ç‰¹å¾ç»´åº¦
DEFAULT_FLAT_DIM = DEFAULT_TOKEN_LEN * DEFAULT_FEAT_DIM  # å±•å¹³åç»´åº¦ = 121

# å‚…é‡Œå¶ç³»æ•°ç»´åº¦è¯´æ˜ï¼ˆå‰ 10 ç»´ç”¨äºæ›²çº¿é‡å»ºï¼Œç¬¬ 11 ç»´ä¸ºé™„åŠ ç‰¹å¾ï¼‰
# [0]: a0 (ç›´æµåˆ†é‡)
# [1-3]: A1, u1, v1 (ä¸€æ¬¡è°æ³¢)
# [4-6]: A2, u2, v2 (äºŒæ¬¡è°æ³¢)
# [7-9]: A3, u3, v3 (ä¸‰æ¬¡è°æ³¢)
# [10]: é™„åŠ ç‰¹å¾ï¼ˆåœ¨æ›²çº¿å¯¼å‡ºæ—¶è¢«å¿½ç•¥ï¼‰
FOURIER_CURVE_DIMS = 10  # ç”¨äºæ›²çº¿é‡å»ºçš„ç»´åº¦æ•°


def safe_torch_load(path, map_location='cpu', weights_only=None):
    """
    å…¼å®¹ä¸åŒ PyTorch ç‰ˆæœ¬çš„ torch.load åŒ…è£…å‡½æ•°
    
    æ—§ç‰ˆ PyTorch ä¸æ”¯æŒ weights_only å‚æ•°
    """
    import torch
    try:
        if weights_only is not None:
            return torch.load(path, map_location=map_location, weights_only=weights_only)
        else:
            return torch.load(path, map_location=map_location)
    except TypeError:
        # æ—§ç‰ˆ PyTorch ä¸æ”¯æŒ weights_only å‚æ•°
        return torch.load(path, map_location=map_location)


def setup_colab_env():
    """Colab ç¯å¢ƒåˆå§‹åŒ–"""
    # æŒ‚è½½ Google Drive
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("âœ“ Google Drive å·²æŒ‚è½½")
        IN_COLAB = True
    except:
        print("âš  é Colab ç¯å¢ƒæˆ– Drive æŒ‚è½½å¤±è´¥")
        IN_COLAB = False
    
    return IN_COLAB


def copy_data_to_local(drive_data_dir: str, local_data_dir: str):
    """
    å°† Drive æ•°æ®å¤åˆ¶åˆ° Colab æœ¬åœ°ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
    
    å…³é”®æ–‡ä»¶ï¼štrain_data.npz, metadata.json, scaler.pkl
    """
    drive_path = Path(drive_data_dir)
    local_path = Path(local_data_dir)
    local_path.mkdir(parents=True, exist_ok=True)
    
    required_files = ['train_data.npz', 'metadata.json', 'scaler.pkl']
    
    print(f"\nå¤åˆ¶æ•°æ®ä» Drive åˆ°æœ¬åœ°...")
    for fname in required_files:
        src = drive_path / fname
        dst = local_path / fname
        if src.exists():
            if not dst.exists():
                print(f"  å¤åˆ¶: {fname}")
                shutil.copy(src, dst)
            else:
                print(f"  è·³è¿‡ (å·²å­˜åœ¨): {fname}")
        else:
            print(f"  âš  ç¼ºå¤±: {fname}")
    
    print(f"âœ“ æ•°æ®å·²å‡†å¤‡åˆ°: {local_path}\n")
    return str(local_path)


def get_device():
    """è·å–å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ“ ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
        print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        return device
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return torch.device('mps')
    else:
        print("âš  ä½¿ç”¨ CPU (è®­ç»ƒä¼šå¾ˆæ…¢)")
        return torch.device('cpu')


def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ï¼ˆåŒ…æ‹¬ä¿å­˜/æ¢å¤ rng stateï¼‰"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def save_rng_state():
    """ä¿å­˜éšæœºçŠ¶æ€"""
    return {
        'numpy': np.random.get_state(),
        'torch': torch.get_rng_state(),
        'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None
    }


def load_rng_state(rng_state):
    """æ¢å¤éšæœºçŠ¶æ€"""
    np.random.set_state(rng_state['numpy'])
    torch.set_rng_state(rng_state['torch'])
    if torch.cuda.is_available() and rng_state['cuda'] is not None:
        torch.cuda.set_rng_state_all(rng_state['cuda'])


def safe_log1p(x):
    """å®‰å…¨çš„log1på˜æ¢,å¤„ç†è´Ÿå€¼ï¼ˆä¸ç¬¬ä¸€ä»½ä»£ç ä¸¥æ ¼ä¸€è‡´ï¼‰- NumPyç‰ˆæœ¬"""
    return np.sign(x) * np.log1p(np.abs(x))


def inverse_log1p(y):
    """log1pçš„é€†å˜æ¢ï¼ˆä¸ç¬¬ä¸€ä»½ä»£ç ä¸¥æ ¼ä¸€è‡´ï¼‰- NumPyç‰ˆæœ¬"""
    return np.sign(y) * (np.exp(np.abs(y)) - 1)


def safe_log1p_torch(x: torch.Tensor) -> torch.Tensor:
    """å®‰å…¨çš„log1på˜æ¢,å¤„ç†è´Ÿå€¼ - PyTorchç‰ˆæœ¬ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰"""
    return torch.sign(x) * torch.log1p(torch.abs(x))


def inverse_log1p_torch(y: torch.Tensor) -> torch.Tensor:
    """
    log1pçš„é€†å˜æ¢ - PyTorchç‰ˆæœ¬ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰
    
    ğŸ”§ AMPç¨³å®šæ€§ä¿®å¤ï¼š
    - å¼ºåˆ¶åœ¨ float32 ä¸‹è®¡ç®—ï¼Œé¿å… float16 ä¸‹ exp() æº¢å‡ºæˆ inf
    - exp(88) å·²æ¥è¿‘ float32 ä¸Šé™ï¼Œfloat16 ä¸Šé™æ›´ä½ (~11)
    """
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿å­˜åŸå§‹dtypeï¼Œå¼ºåˆ¶ float32 è®¡ç®—
    original_dtype = y.dtype
    y = y.float()  # å¼ºåˆ¶ float32
    
    # ğŸ”§ é™åˆ¶ abs(y) æœ€å¤§å€¼ï¼Œé¿å… exp æº¢å‡º
    # float32 å®‰å…¨èŒƒå›´: exp(87) â‰ˆ 6e37, exp(88) â‰ˆ inf
    y_abs_clamped = torch.clamp(torch.abs(y), max=80.0)
    
    result = torch.sign(y) * (torch.exp(y_abs_clamped) - 1)
    
    return result.to(original_dtype)


class ScalerAdapter:
    """Scaleré€‚é…å™¨ï¼šå¯¹é½ç¬¬ä¸€ä»½ä»£ç çš„scaler.pklæ ¼å¼"""
    
    def __init__(self, scaler_path: str):
        scaler_dict = joblib.load(scaler_path)
        
        self.mean = scaler_dict['mean']
        self.std = scaler_dict['std']
        self.log1p_dims = scaler_dict.get('log1p_dims', [0, 1, 4, 7, 10])
        self.fitted = scaler_dict.get('fitted', True)
        
    def _apply_log1p(self, X: np.ndarray) -> np.ndarray:
        X_transformed = X.copy()
        for dim in self.log1p_dims:
            X_transformed[:, :, dim] = safe_log1p(X_transformed[:, :, dim])
        return X_transformed
    
    def _inverse_log1p(self, X: np.ndarray) -> np.ndarray:
        X_original = X.copy()
        for dim in self.log1p_dims:
            X_original[:, :, dim] = inverse_log1p(X_original[:, :, dim])
        return X_original
    
    def transform(self, X_tokens: np.ndarray) -> np.ndarray:
        if X_tokens.ndim != 3 or X_tokens.shape[1:] != (11, 11):
            raise ValueError(f"Expected shape (N, 11, 11), got {X_tokens.shape}")
        X_log = self._apply_log1p(X_tokens)
        X_scaled = (X_log - self.mean) / self.std
        return X_scaled
    
    def inverse_transform(self, X_scaled: np.ndarray) -> np.ndarray:
        if X_scaled.ndim != 3 or X_scaled.shape[1:] != (11, 11):
            raise ValueError(f"Expected shape (N, 11, 11), got {X_scaled.shape}")
        X_log = X_scaled * self.std + self.mean
        X_original = self._inverse_log1p(X_log)
        return X_original
    
    def inverse_transform_torch(self, X_scaled: torch.Tensor) -> torch.Tensor:
        """PyTorchç‰ˆæœ¬çš„é€†å˜æ¢ï¼ˆä¿ç•™æ¢¯åº¦ï¼Œç”¨äºç‰©ç†ç©ºé—´åˆ†ç¦»æŸå¤±ï¼‰"""
        if X_scaled.ndim != 3 or X_scaled.shape[1:] != (11, 11):
            raise ValueError(f"Expected shape (N, 11, 11), got {X_scaled.shape}")
        
        device = X_scaled.device
        mean_torch = torch.from_numpy(self.mean).float().to(device)
        std_torch = torch.from_numpy(self.std).float().to(device)
        
        # åæ ‡å‡†åŒ–
        X_log = X_scaled * std_torch + mean_torch
        
        # ålog1på˜æ¢
        X_original = X_log.clone()
        for dim in self.log1p_dims:
            X_original[:, :, dim] = inverse_log1p_torch(X_log[:, :, dim])
        
        return X_original


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.log_file = output_dir / 'train_log.csv'
        self.logs = []
        
        if not self.log_file.exists():
            pd.DataFrame(columns=[
                'timestamp', 'stage', 'epoch', 'loss', 
                'loss_rec', 'loss_balance', 'loss_sep', 'loss_sparse',
                'loss_sep_phys', 'loss_kl', 'loss_entropy'
            ]).to_csv(self.log_file, index=False)
        else:
            # æ¢å¤å·²æœ‰æ—¥å¿—
            existing_logs = pd.read_csv(self.log_file)
            self.logs = existing_logs.to_dict('records')
    
    def log(self, stage: str, epoch: int, loss_dict: Dict[str, float]):
        log_entry = {
            'timestamp': datetime.datetime.now().isoformat(),
            'stage': stage,
            'epoch': epoch,
            'loss': loss_dict.get('total', 0.0),
            'loss_rec': loss_dict.get('rec', 0.0),
            'loss_balance': loss_dict.get('balance', 0.0),
            'loss_sep': loss_dict.get('sep', 0.0),
            'loss_sparse': loss_dict.get('sparse', 0.0),
            'loss_sep_phys': loss_dict.get('sep_phys', 0.0),
            'loss_kl': loss_dict.get('kl', 0.0),
            'loss_entropy': loss_dict.get('entropy', 0.0)
        }
        
        self.logs.append(log_entry)
        
        pd.DataFrame([log_entry]).to_csv(
            self.log_file, 
            mode='a', 
            header=False, 
            index=False
        )
    
    def save_config(self, config: Dict):
        config_file = self.output_dir / 'run_config.json'
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2, default=str)
    
    def plot_training_curves(self):
        if not self.logs:
            return
        
        df = pd.DataFrame(self.logs)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Loss Curves', fontsize=16)
        
        for stage in df['stage'].unique():
            stage_df = df[df['stage'] == stage]
            
            axes[0, 0].plot(stage_df['epoch'], stage_df['loss'], 
                           label=f'Stage {stage}', marker='o', markersize=3)
            
            if stage_df['loss_rec'].sum() > 0:
                axes[0, 1].plot(stage_df['epoch'], stage_df['loss_rec'],
                               label=f'Stage {stage}', marker='o', markersize=3)
            
            if stage_df['loss_balance'].sum() > 0:
                axes[1, 0].plot(stage_df['epoch'], stage_df['loss_balance'],
                               label=f'Stage {stage} Balance', marker='o', markersize=3)
            if stage_df['loss_sep'].sum() > 0:
                axes[1, 0].plot(stage_df['epoch'], stage_df['loss_sep'],
                               label=f'Stage {stage} Sep', marker='o', markersize=3, linestyle='--')
            
            if stage_df['loss_sep_phys'].sum() > 0:
                axes[1, 1].plot(stage_df['epoch'], stage_df['loss_sep_phys'],
                               label=f'Stage {stage} Phys Sep', marker='o', markersize=3)
            if stage_df['loss_kl'].sum() > 0:
                axes[1, 1].plot(stage_df['epoch'], stage_df['loss_kl'],
                               label=f'Stage {stage} KL', marker='o', markersize=3)
        
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        axes[0, 1].set_title('Reconstruction Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        axes[1, 0].set_title('Balance & Separation')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        axes[1, 1].set_title('Physical Sep / Distillation')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)
        plt.close()
        
        print(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {self.output_dir / 'training_curves.png'}")


# ============================================================================
# 1. æ•°æ®åŠ è½½ä¸åˆ†å±‚é‡‡æ · (Colabä¼˜åŒ–)
# ============================================================================

class DeepAATokenDataset(Dataset):
    
    def __init__(
        self, 
        X_tokens, 
        metadata=None, 
        transform=None,
        use_memmap=False,
        memmap_path=None
    ):

        self.transform = transform
        self.metadata = metadata or {}
        
        # è‡ªåŠ¨å†³ç­–æ˜¯å¦ä½¿ç”¨ memmapï¼ˆæ•°æ®é‡ > 1GBï¼‰
        estimated_size_gb = X_tokens.nbytes / 1e9 if hasattr(X_tokens, 'nbytes') else 0
        auto_memmap = estimated_size_gb > 1.0
        
        if use_memmap or auto_memmap:
            if memmap_path is None:
                import tempfile
                memmap_path = tempfile.mktemp(suffix='.dat')
            
            # ä¿å­˜åˆ° memmap
            if not isinstance(X_tokens, np.memmap):
                shape = X_tokens.shape
                dtype = X_tokens.dtype
                memmap_data = np.memmap(
                    memmap_path, 
                    dtype=dtype, 
                    mode='w+', 
                    shape=shape
                )
                memmap_data[:] = X_tokens[:]
                memmap_data.flush()
                self.X_tokens = np.memmap(
                    memmap_path, 
                    dtype=dtype, 
                    mode='r', 
                    shape=shape
                )
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° memmap: {memmap_path} ({estimated_size_gb:.2f} GB)")
            else:
                self.X_tokens = X_tokens
        else:
            # ç¡®ä¿æ˜¯ float32ï¼ˆèŠ‚çœä¸€åŠå†…å­˜ï¼‰
            if X_tokens.dtype == np.float64:
                X_tokens = X_tokens.astype(np.float32)
            self.X_tokens = X_tokens
        
        self.n_samples = len(self.X_tokens)
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        """æƒ°æ€§åŠ è½½å•æ¡æ•°æ®"""
        # ä» memmap è¯»å–å•æ¡ï¼ˆä¸ä¼šåŠ è½½å…¨éƒ¨ï¼‰
        x = self.X_tokens[idx].copy()  # copy é¿å…åªè¯»é—®é¢˜
        
        if self.transform:
            x = self.transform(x)
        
        # è½¬ torch tensorï¼ˆä»…å•æ¡ï¼‰
        x_tensor = torch.from_numpy(x).float()
        
        sample = {'X': x_tensor, 'idx': idx}
        
        # æ·»åŠ å…ƒæ•°æ®
        for key in ['aez', 'climate', 'region', 'year']:
            if key in self.metadata:
                val = self.metadata[key][idx]
                sample[key] = torch.tensor(val) if not isinstance(val, torch.Tensor) else val
        
        return sample


class StratifiedBatchSampler(Sampler):
    
    def __init__(self, 
                 aez_labels: np.ndarray,
                 batch_size: int,
                 samples_per_stratum: Optional[int] = None,
                 shuffle: bool = True,
                 n_batches_per_epoch: Optional[int] = None,
                 warn_if_too_many_strata: bool = True):
        """
        å‚æ•°:
            aez_labels: (N,) åˆ†å±‚æ ‡ç­¾ (å¯ä»¥æ˜¯ AEZ æˆ– AEZ*100+climate_zone)
            batch_size: æ‰¹æ¬¡å¤§å°
            samples_per_stratum: æ¯å±‚æ¯æ‰¹æŠ½æ ·æ•°
            shuffle: æ˜¯å¦æ‰“ä¹±
            n_batches_per_epoch: æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•° (å…³é”®:æ§åˆ¶è®­ç»ƒæ—¶é—´)
            warn_if_too_many_strata: å½“ strata æ•°é‡è¿‡å¤šæ—¶æ˜¯å¦è­¦å‘Š
        """
        self.aez_labels = aez_labels
        self.batch_size = batch_size
        self.shuffle = shuffle
        
        # æŒ‰åˆ†å±‚åˆ†ç»„ç´¢å¼•
        self.stratum_indices = defaultdict(list)
        for idx, aez in enumerate(aez_labels):
            self.stratum_indices[aez].append(idx)
        
        self.strata = list(self.stratum_indices.keys())
        self.n_strata = len(self.strata)
        
        # æ¯å±‚æ¯æ‰¹æŠ½æ ·æ•°
        if samples_per_stratum is None:
            self.samples_per_stratum = max(1, batch_size // self.n_strata)
        else:
            self.samples_per_stratum = samples_per_stratum
        
        # æ£€æŸ¥è¾¹ç•Œæ¡ä»¶ï¼šstrata æ•°é‡æ˜¯å¦è¶…è¿‡ batch_size
        min_samples_needed = self.n_strata * self.samples_per_stratum
        if min_samples_needed > batch_size:
            if warn_if_too_many_strata:
                warnings.warn(
                    f"åˆ†å±‚æ•° ({self.n_strata}) Ã— æ¯å±‚æ ·æœ¬æ•° ({self.samples_per_stratum}) = {min_samples_needed} "
                    f"> batch_size ({batch_size})ã€‚\\n"
                    f"æ¯ä¸ª batch å°†åªè¦†ç›–éƒ¨åˆ† strataï¼ˆçº¦ {batch_size // self.samples_per_stratum} å±‚ï¼‰ï¼Œ"
                    f"ä½¿ç”¨è½®æ¢ç­–ç•¥ç¡®ä¿é•¿æœŸå‡è¡¡ã€‚"
                )
            # ä½¿ç”¨è½®æ¢ç­–ç•¥ï¼šæ¯ä¸ª batch åªé‡‡æ ·éƒ¨åˆ† strata
            self.strata_per_batch = batch_size // self.samples_per_stratum
            self.use_rotation = True
        else:
            self.strata_per_batch = self.n_strata
            self.use_rotation = False
        
        # æ¯epochæ‰¹æ¬¡æ•° (å…³é”®ï¼šæ§åˆ¶è®­ç»ƒæ—¶é—´)
        if n_batches_per_epoch is not None:
            self.n_batches = n_batches_per_epoch
        else:
            # é»˜è®¤ï¼šåŸºäºæ€»æ ·æœ¬æ•°
            total_samples = len(aez_labels)
            self.n_batches = total_samples // batch_size
        
        print(f"  åˆ†å±‚é‡‡æ ·: {self.n_strata} å±‚, æ¯epoch {self.n_batches} æ‰¹æ¬¡, æ¯æ‰¹æ¬¡ç›®æ ‡å¤§å°: {self.batch_size}")
        if self.use_rotation:
            print(f"  âš  ä½¿ç”¨è½®æ¢ç­–ç•¥: æ¯æ‰¹æ¬¡è¦†ç›– {self.strata_per_batch} å±‚")
        
    def __iter__(self):
        # ä¸ºæ¯å±‚åˆ›å»ºå¾ªç¯è¿­ä»£å™¨
        stratum_iters = {}
        for aez, indices in self.stratum_indices.items():
            indices_copy = indices.copy()
            if self.shuffle:
                np.random.shuffle(indices_copy)
            stratum_iters[aez] = iter(self._cycle_indices(indices_copy, self.shuffle))
        
        # å¦‚æœä½¿ç”¨è½®æ¢ç­–ç•¥ï¼Œæ‰“ä¹± strata é¡ºåº
        strata_order = self.strata.copy()
        if self.shuffle:
            np.random.shuffle(strata_order)
        strata_cycle = iter(self._cycle_indices(strata_order, self.shuffle))
        
        # ç”Ÿæˆæ‰¹æ¬¡
        batches_yielded = 0
        for batch_idx in range(self.n_batches):
            batch = []
            
            if self.use_rotation:
                # è½®æ¢ç­–ç•¥ï¼šæ¯ä¸ª batch åªè¦†ç›–éƒ¨åˆ† strata
                batch_strata = [next(strata_cycle) for _ in range(self.strata_per_batch)]
                for aez in batch_strata:
                    for _ in range(self.samples_per_stratum):
                        idx = next(stratum_iters[aez])
                        batch.append(idx)
            else:
                # æ ‡å‡†ç­–ç•¥ï¼šæ¯ä¸ª batch è¦†ç›–æ‰€æœ‰ strata
                for aez in self.strata:
                    for _ in range(self.samples_per_stratum):
                        idx = next(stratum_iters[aez])  # _cycle_indices ä¿è¯ä¸ä¼š StopIteration
                        batch.append(idx)
            
            # è¡¥å……åˆ° batch_size (å¾ªç¯é‡‡æ ·ä¿è¯ä¸€å®šèƒ½å¡«æ»¡)
            while len(batch) < self.batch_size:
                aez = next(strata_cycle) if self.use_rotation else self.strata[batch_idx % self.n_strata]
                idx = next(stratum_iters[aez])
                batch.append(idx)
            
            # æ‰“ä¹±å¹¶æˆªå–
            if self.shuffle:
                np.random.shuffle(batch)
            
            yield batch[:self.batch_size]
            batches_yielded += 1
        
        # éªŒè¯ï¼šç¡®ä¿ç”Ÿæˆäº†æ­£ç¡®æ•°é‡çš„æ‰¹æ¬¡
        if batches_yielded != self.n_batches:
            print(f"  âš  è­¦å‘Š: æœŸæœ› {self.n_batches} æ‰¹æ¬¡, å®é™…ç”Ÿæˆ {batches_yielded} æ‰¹æ¬¡")
    
    def _cycle_indices(self, indices, shuffle):
        while True:
            if shuffle:
                np.random.shuffle(indices)
            for idx in indices:
                yield idx
    
    def __len__(self):
        return self.n_batches


# ============================================================================
# 2. Entmaxæ¿€æ´»å‡½æ•° (ä¼˜åŒ–: n_iterå¯é…ç½®)
# ============================================================================

class Entmax15(nn.Module):
    """Entmax 1.5æ¿€æ´»å‡½æ•° (n_iterå¯é…ç½®)"""
    
    def __init__(self, dim=-1, n_iter=15):
        super().__init__()
        self.dim = dim
        self.n_iter = n_iter  # é»˜è®¤15ï¼Œæ¯”50å¿«å¾ˆå¤š
    
    def forward(self, logits):
        return entmax15(logits, dim=self.dim, n_iter=self.n_iter)


def entmax15(logits, dim=-1, n_iter=15):
    """
    Entmax 1.5 çš„è¿‘ä¼¼å®ç° (n_iterå¯é…ç½®)
    
    âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è¿‘ä¼¼ç‰ˆæœ¬ï¼Œä¸æ˜¯æ ‡å‡† entmax 1.5ï¼š
    - ä½¿ç”¨ softmax åˆå§‹åŒ– + è¿­ä»£æŠ•å½±
    - ç¨€ç–æ€§å’Œæ•°å€¼ç¨³å®šæ€§å¯èƒ½ä¸æ ‡å‡†å®ç°æœ‰å·®å¼‚
    - å¦‚éœ€ä¸¥æ ¼çš„ entmax 1.5ï¼Œå»ºè®®ä½¿ç”¨ entmax åŒ…çš„ entmax15 / entmax_bisect
    
    å‚æ•°:
        logits: è¾“å…¥å¼ é‡
        dim: å½’ä¸€åŒ–ç»´åº¦
        n_iter: è¿­ä»£æ¬¡æ•°ï¼ˆå¢åŠ å¯æé«˜ç²¾åº¦ä½†é™ä½é€Ÿåº¦ï¼‰
    
    è¿”å›:
        è¿‘ä¼¼çš„ entmax 1.5 è¾“å‡ºï¼ˆç¨€ç–æ¦‚ç‡åˆ†å¸ƒï¼‰
    
    ğŸ”§ AMPç¨³å®šæ€§ä¿®å¤ï¼š
    - å¼ºåˆ¶åœ¨ float32 ä¸‹è®¡ç®—ï¼Œé¿å… float16 ç²¾åº¦ä¸è¶³å¯¼è‡´çš„ sqrt(0)ã€1/alpha_sqrt=inf é—®é¢˜
    - eps æå‡åˆ° 1e-6ï¼Œé€‚é… float16 æœ‰æ•ˆç²¾åº¦
    """
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿å­˜åŸå§‹dtypeï¼Œå¼ºåˆ¶è½¬ä¸º float32 è®¡ç®—
    original_dtype = logits.dtype
    logits = logits.float()  # å¼ºåˆ¶ float32
    
    alpha = F.softmax(logits, dim=dim)
    
    # ğŸ”§ æé«˜ eps åˆ° 1e-6ï¼Œfloat16 ä¸‹ 1e-12 ä¼šç›´æ¥å˜æˆ 0
    eps = 1e-6
    
    for _ in range(n_iter):
        alpha_sqrt = torch.sqrt(torch.clamp(alpha, min=eps))
        tau = (torch.sum(alpha_sqrt, dim=dim, keepdim=True) - 1) / \
              torch.sum(1.0 / torch.clamp(alpha_sqrt, min=eps), dim=dim, keepdim=True)
        
        alpha_new = torch.clamp(
            (alpha_sqrt + tau) ** 2,
            min=0.0
        )
        
        # ğŸ”§ å½’ä¸€åŒ–æ—¶ä¹ŸåŠ  eps ä¿æŠ¤
        alpha = alpha_new / (torch.sum(alpha_new, dim=dim, keepdim=True) + eps)
    
    # ğŸ”§ è¿”å›å‰è½¬å›åŸå§‹ dtype
    return alpha.to(original_dtype)


# ============================================================================
# 3. æ¨¡å‹å®šä¹‰ (æ”¯æŒAMP)
# ============================================================================

class TokenEmbedding(nn.Module):
    def __init__(self, input_dim=11, d_model=256):
        super().__init__()
        self.proj = nn.Linear(input_dim, d_model)
        self.norm = nn.LayerNorm(d_model)
    
    def forward(self, x):
        x = self.proj(x)
        x = self.norm(x)
        return x


class TransformerEncoder(nn.Module):
    def __init__(self, 
                 d_model=256, 
                 n_heads=8, 
                 n_layers=4,
                 dim_feedforward=1024,
                 dropout=0.1):
        super().__init__()
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        
        self.encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=n_layers
        )
        
    def forward(self, x, mask=None):
        if mask is not None:
            src_key_padding_mask = mask
        else:
            src_key_padding_mask = None
        
        out = self.encoder(x, src_key_padding_mask=src_key_padding_mask)
        return out


class PrototypeMixingHead(nn.Module):
    def __init__(self, d_model=256, n_prototypes=50, use_entmax=True, entmax_n_iter=15):
        super().__init__()
        
        self.pooling = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Linear(d_model, d_model)
        )
        
        self.to_logits = nn.Linear(d_model, n_prototypes)
        
        self.use_entmax = use_entmax
        if use_entmax:
            self.activation = Entmax15(dim=-1, n_iter=entmax_n_iter)
        else:
            self.activation = nn.Softmax(dim=-1)
    
    def forward(self, z):
        z_global = torch.mean(z, dim=1)
        z_global = self.pooling(z_global)
        logits = self.to_logits(z_global)
        w = self.activation(logits)
        return w, logits


class GenerativeDecoder(nn.Module):
    def __init__(self, d_model=256, output_dim=11):
        super().__init__()
        
        self.expand = nn.Linear(d_model, 11 * d_model)
        
        decoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=8,
            dim_feedforward=1024,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        
        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=2)
        self.to_tokens = nn.Linear(d_model, output_dim)
        
    def forward(self, z_hat):
        B = z_hat.size(0)
        x = self.expand(z_hat)
        x = x.view(B, 11, -1)
        x = self.decoder(x)
        X_hat = self.to_tokens(x)
        return X_hat


class DeepAABigModel(nn.Module):
    """DeepAAå¤§æ¨¡å‹ (æ”¯æŒAMP)"""
    
    def __init__(self,
                 n_prototypes=50,
                 d_model=256,
                 n_heads=8,
                 n_encoder_layers=4,
                 use_entmax=True,
                 entmax_n_iter=15):
        super().__init__()
        
        self.n_prototypes = n_prototypes
        self.d_model = d_model
        
        self.token_embedding = TokenEmbedding(input_dim=11, d_model=d_model)
        self.encoder = TransformerEncoder(
            d_model=d_model,
            n_heads=n_heads,
            n_layers=n_encoder_layers
        )
        self.prototype_head = PrototypeMixingHead(
            d_model=d_model,
            n_prototypes=n_prototypes,
            use_entmax=use_entmax,
            entmax_n_iter=entmax_n_iter
        )
        self.decoder = GenerativeDecoder(d_model=d_model, output_dim=11)
        
        self.prototypes = nn.Parameter(torch.randn(n_prototypes, d_model))
        nn.init.orthogonal_(self.prototypes)
        
        self.mask_token = nn.Parameter(torch.randn(1, 1, 11))
        self.mask_decoder_head = nn.Linear(d_model, 11)
    
    def forward(self, X, mask=None, stage='B'):
        B = X.size(0)
        
        if mask is not None:
            X = X.clone()
            num_masked = mask.sum().item()
            X[mask] = self.mask_token.squeeze(0).expand(num_masked, -1)
        
        z = self.token_embedding(X)
        # Masked reconstruction should not treat masked tokens as padding.
        # We only replace their values with mask_token; attention should still
        # operate over the full sequence.
        z = self.encoder(z, mask=None)
        
        if stage == 'A':
            token_preds = self.mask_decoder_head(z)
            return {'X_hat': token_preds, 'z': z}
        else:
            w, logits = self.prototype_head(z)
            z_hat = torch.matmul(w, self.prototypes)
            X_hat = self.decoder(z_hat)
            
            return {
                'X_hat': X_hat,
                'w': w,
                'logits': logits,
                'z': z,
                'z_hat': z_hat
            }
    
    def generate_from_prototype(self, k):
        with torch.no_grad():
            p_k = self.prototypes[k:k+1]
            X_hat_k = self.decoder(p_k)
        return X_hat_k.squeeze(0)


# ============================================================================
# 4. è®­ç»ƒé˜¶æ®µ (å¸¦AMPå’ŒCheckpoint)
# ============================================================================

class MaskedReconstructionTrainer:
    """é˜¶æ®µAè®­ç»ƒå™¨: Masked Reconstruction (å¸¦AMP)"""
    
    def __init__(self, 
                 model: DeepAABigModel,
                 device: torch.device,
                 mask_ratio: float = 0.3,
                 use_amp: bool = True):
        self.model = model
        self.device = device
        self.mask_ratio = mask_ratio
        self.use_amp = use_amp and device.type == 'cuda'
        
        # å§‹ç»ˆåˆ›å»º scaler å±æ€§ï¼Œé¿å… AttributeError
        self.scaler = GradScaler() if self.use_amp else None
    
    def create_random_mask(self, batch_size, n_tokens=11):
        mask = torch.rand(batch_size, n_tokens) < self.mask_ratio
        mask[mask.sum(dim=1) == 0, 0] = True
        return mask.to(self.device)
    
    def train_epoch(self, dataloader, optimizer, epoch):
        self.model.train()
        total_loss = 0
        n_batches = 0
        expected_batches = len(dataloader)
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Stage A]", total=expected_batches)
        
        for batch in pbar:
            X = batch['X'].to(self.device)
            B = X.size(0)
            
            mask = self.create_random_mask(B)
            
            optimizer.zero_grad()
            
            if self.use_amp:
                with autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):
                    outputs = self.model(X, mask=mask, stage='A')
                    X_hat = outputs['X_hat']
                    loss = F.mse_loss(X_hat[mask], X[mask])
                
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                outputs = self.model(X, mask=mask, stage='A')
                X_hat = outputs['X_hat']
                loss = F.mse_loss(X_hat[mask], X[mask])
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'batch': f'{n_batches}/{expected_batches}'})
        
        # éªŒè¯å®é™…æ‰¹æ¬¡æ•°
        if n_batches != expected_batches:
            print(f"\n  âš  è­¦å‘Š: æœŸæœ› {expected_batches} æ‰¹æ¬¡, å®é™…è®­ç»ƒ {n_batches} æ‰¹æ¬¡")
        
        return total_loss / n_batches


class PrototypeMixingTrainer:
    """é˜¶æ®µBè®­ç»ƒå™¨: åŸå‹æ··åˆ (å¸¦AMP)"""
    
    def __init__(self,
                 model: DeepAABigModel,
                 device: torch.device,
                 lambda_balance: float = 0.01,
                 lambda_sep_latent: float = 0.001,
                 lambda_sparse: float = 0.0,  # å…³é—­ sparse lossï¼Œé¿å…è¿‡åº¦å¹³æ»‘
                 warmup_epochs: int = 5,
                 use_amp: bool = True):
        self.model = model
        self.device = device
        self.lambda_balance = lambda_balance
        self.lambda_sep_latent = lambda_sep_latent
        self.lambda_sparse = lambda_sparse
        self.warmup_epochs = warmup_epochs
        self.use_amp = use_amp and device.type == 'cuda'
        
        # å§‹ç»ˆåˆ›å»º scaler å±æ€§ï¼Œé¿å… AttributeError
        self.scaler = GradScaler() if self.use_amp else None
        
        # ç¼“å­˜ separation lossï¼ˆä¸ batch æ— å…³ï¼Œæ¯ epoch åªéœ€è®¡ç®—ä¸€æ¬¡ï¼‰
        self._cached_sep_loss = None
    
    def compute_balance_loss(self, w):
        """
        ä½¿ç”¨å¯¹ç§° KL æ•£åº¦é¼“åŠ±åŸå‹ä½¿ç”¨å‡è¡¡ï¼Œä½†ä¸å¼ºåˆ¶å®Œå…¨å‡åŒ€
        
        ğŸ”§ AMPç¨³å®šæ€§ä¿®å¤ï¼šeps ä» 1e-8 æå‡åˆ° 1e-6ï¼Œé€‚é… float16 ç²¾åº¦
        """
        # ğŸ”§ eps æå‡åˆ° 1e-6ï¼Œfloat16 ä¸‹ 1e-8 å¯èƒ½ç›´æ¥å˜æˆ 0
        eps = 1e-6
        
        mean_w = w.mean(dim=0)
        uniform = torch.ones_like(mean_w) / self.model.n_prototypes
        
        # å¯¹ç§° KL: (KL(mean_w || uniform) + KL(uniform || mean_w)) / 2
        kl_forward = F.kl_div(
            (mean_w + eps).log(),
            uniform,
            reduction='batchmean'
        )
        kl_backward = F.kl_div(
            uniform.log(),
            mean_w + eps,
            reduction='batchmean'
        )
        loss_balance = (kl_forward + kl_backward) / 2
        return loss_balance
    
    def compute_separation_loss_latent(self):
        """é¼“åŠ±åŸå‹åœ¨æ½œåœ¨ç©ºé—´åˆ†ç¦»ï¼Œä½†ä¸è¦æ±‚å®Œå…¨æ­£äº¤"""
        P = self.model.prototypes
        K = P.size(0)
        P_norm = F.normalize(P, dim=1)
        sim_matrix = torch.matmul(P_norm, P_norm.t())
        mask = ~torch.eye(K, dtype=torch.bool, device=self.device)
        similarities = sim_matrix[mask]
        
        # é™ä½ marginï¼š0.7 å¯¹åº”çº¦ 45Â° è§’åº¦ï¼Œæ¯” 0.5 (60Â°) æ›´å®½æ¾
        margin = 0.7
        loss_sep = torch.clamp(similarities - margin, min=0).mean()
        return loss_sep
    
    def compute_sparse_loss(self, w):
        """
        ç†µæ­£åˆ™åŒ–ï¼šæ§åˆ¶åŸå‹åˆ†é…çš„åˆ†æ•£ç¨‹åº¦
        
        è¿”å›è´Ÿçš„å½’ä¸€åŒ–ç†µã€‚å½“lambda_sparse > 0æ—¶ï¼š
        - æœ€å°åŒ–è´Ÿç†µ = æœ€å¤§åŒ–ç†µ = é¼“åŠ±æ›´å¹³æ»‘/åˆ†æ•£çš„åŸå‹ä½¿ç”¨
        - é¿å…æ¨¡å‹åªä¾èµ–å°‘æ•°å‡ ä¸ªåŸå‹
        
        å¦‚æœéœ€è¦ç¨€ç–æ€§ï¼ˆé›†ä¸­ä½¿ç”¨å°‘æ•°åŸå‹ï¼‰ï¼Œåº”è®¾ç½® lambda_sparse < 0
        
        ğŸ”§ AMPç¨³å®šæ€§ä¿®å¤ï¼šeps ä» 1e-8 æå‡åˆ° 1e-6
        """
        # ğŸ”§ eps æå‡åˆ° 1e-6ï¼Œfloat16 ä¸‹ 1e-8 å¯èƒ½ç›´æ¥å˜æˆ 0
        eps = 1e-6
        entropy = -(w * (w + eps).log()).sum(dim=1).mean()
        max_entropy = math.log(self.model.n_prototypes)
        
        normalized_entropy = entropy / max_entropy
        return -normalized_entropy  # è¿”å›è´Ÿç†µä¾›lossä½¿ç”¨
    
    def train_epoch(self, dataloader, optimizer, epoch):
        self.model.train()
        total_loss = 0
        loss_dict = defaultdict(float)
        n_batches = 0
        expected_batches = len(dataloader)
        
        if epoch < self.warmup_epochs:
            alpha_sep = self.lambda_sep_latent * (epoch / self.warmup_epochs)
            alpha_sparse = self.lambda_sparse * (epoch / self.warmup_epochs)
        else:
            alpha_sep = self.lambda_sep_latent
            alpha_sparse = self.lambda_sparse
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Stage B]", total=expected_batches)
        
        for batch in pbar:
            X = batch['X'].to(self.device)
            
            optimizer.zero_grad()
            
            if self.use_amp:
                with autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):
                    outputs = self.model(X, stage='B')
                    X_hat = outputs['X_hat']
                    w = outputs['w']
                    
                    loss_rec = F.mse_loss(X_hat, X)
                    loss_balance = self.compute_balance_loss(w)
                    loss_sep = self.compute_separation_loss_latent()
                    loss_sparse = self.compute_sparse_loss(w)
                    
                    loss = (loss_rec + 
                           self.lambda_balance * loss_balance +
                           alpha_sep * loss_sep +
                           alpha_sparse * loss_sparse)
                
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                outputs = self.model(X, stage='B')
                X_hat = outputs['X_hat']
                w = outputs['w']
                
                loss_rec = F.mse_loss(X_hat, X)
                loss_balance = self.compute_balance_loss(w)
                loss_sep = self.compute_separation_loss_latent()
                loss_sparse = self.compute_sparse_loss(w)
                
                loss = (loss_rec + 
                       self.lambda_balance * loss_balance +
                       alpha_sep * loss_sep +
                       alpha_sparse * loss_sparse)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
            
            total_loss += loss.item()
            loss_dict['rec'] += loss_rec.item()
            loss_dict['balance'] += loss_balance.item()
            loss_dict['sep'] += loss_sep.item()
            loss_dict['sparse'] += loss_sparse.item()
            n_batches += 1
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'rec': f'{loss_rec.item():.4f}',
                'batch': f'{n_batches}/{expected_batches}'
            })
        
        # éªŒè¯å®é™…æ‰¹æ¬¡æ•°
        if n_batches != expected_batches:
            print(f"\n  âš  è­¦å‘Š: æœŸæœ› {expected_batches} æ‰¹æ¬¡, å®é™…è®­ç»ƒ {n_batches} æ‰¹æ¬¡")
        
        avg_loss = total_loss / n_batches
        avg_loss_dict = {k: v / n_batches for k, v in loss_dict.items()}
        
        return avg_loss, avg_loss_dict


class PhysicalSeparationTrainer:
    """é˜¶æ®µCè®­ç»ƒå™¨: ç‰©ç†ç©ºé—´å»é‡ (å¸¦AMP) - ä¿®å¤ç‰ˆ"""
    
    def __init__(self,
                 model: DeepAABigModel,
                 device: torch.device,
                 scaler_path: str,
                 lambda_sep_phys: float = 0.05,  # é™ä½æƒé‡é¿å…è¿‡åº¦æƒ©ç½š
                 margin_phys: float = 10,  # é™ä½ margin è¦æ±‚
                 top_k_neighbors: int = 3,  # åªå…³æ³¨æœ€è¿‘çš„3ä¸ªé‚»å±…
                 warmup_epochs: int = 3,  # æ·»åŠ  warmup
                 use_amp: bool = True):
        self.model = model
        self.device = device
        self.lambda_sep_phys = lambda_sep_phys
        self.margin_phys = margin_phys
        self.top_k_neighbors = top_k_neighbors
        self.warmup_epochs = warmup_epochs
        self.use_amp = use_amp and device.type == 'cuda'
        
        # å§‹ç»ˆåˆ›å»º scaler å±æ€§ï¼Œé¿å… AttributeError
        self.scaler = GradScaler() if self.use_amp else None
        
        self.scaler_adapter = ScalerAdapter(scaler_path)
        
        # ç¼“å­˜ separation lossï¼ˆä¸ batch æ— å…³ï¼Œæ¯ epoch åªéœ€è®¡ç®—ä¸€æ¬¡ï¼‰
        self._cached_sep_loss = None
    
    def generate_physical_prototypes_with_grad(self):
        """ç”Ÿæˆç‰©ç†ç©ºé—´åŸå‹ - çœŸæ­£çš„ç‰©ç†ç©ºé—´ç‰ˆæœ¬ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰"""
        K = self.model.n_prototypes
        
        # 1. é€šè¿‡decoderç”Ÿæˆæ ‡å‡†åŒ–ç©ºé—´çš„token
        X_proto_scaled = self.model.decoder(self.model.prototypes)  # (K, 11, 11)
        
        # 2. ä½¿ç”¨torchç‰ˆæœ¬çš„scaleré€†å˜æ¢åˆ°çœŸæ­£çš„ç‰©ç†ç©ºé—´
        X_proto_phys = self.scaler_adapter.inverse_transform_torch(X_proto_scaled)  # (K, 11, 11)
        
        # 3. å±•å¹³ä¸ºå‘é‡ç”¨äºè·ç¦»è®¡ç®—
        X_proto_flat = X_proto_phys.view(K, -1)  # (K, 121)
        
        return X_proto_flat
    
    def compute_physical_separation_loss(self):
        """
        è®¡ç®—ç‰©ç†åˆ†ç¦»æŸå¤± - çœŸæ­£åœ¨ç‰©ç†ç©ºé—´è®¡ç®—è·ç¦»
        
        ğŸ”§ AMPç¨³å®šæ€§ä¿®å¤ï¼š
        - å¼ºåˆ¶åœ¨ float32 ä¸‹è®¡ç®—ï¼Œé¿å… float16 ä¸‹ cdist/exp æº¢å‡º
        - inverse_log1p_torch å·²ç»å†…éƒ¨å¼ºåˆ¶ float32
        """
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ float32 è®¡ç®—
        # ç”Ÿæˆç‰©ç†ç©ºé—´åŸå‹ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰
        X_proto_phys = self.generate_physical_prototypes_with_grad().float()  # å¼ºåˆ¶ float32
        K = X_proto_phys.size(0)
        
        # ğŸ”§ è®¡ç®—ç‰©ç†ç©ºé—´è·ç¦»çŸ©é˜µï¼ˆæ¬§æ°è·ç¦»ï¼‰- åœ¨ float32 ä¸‹è®¡ç®—
        dist_matrix = torch.cdist(X_proto_phys, X_proto_phys, p=2)
        
        # æ’é™¤å¯¹è§’çº¿
        mask = ~torch.eye(K, dtype=torch.bool, device=self.device)
        dist_matrix_masked = dist_matrix.clone()
        dist_matrix_masked[~mask] = 1e6  # å¯¹è§’çº¿è®¾ä¸ºå¤§å€¼
        
        # æ‰¾æœ€è¿‘çš„ k ä¸ªé‚»å±…
        topk_dists, _ = torch.topk(
            dist_matrix_masked,
            k=min(self.top_k_neighbors, K-1),
            dim=1,
            largest=False
        )
        
        # Hinge loss: å¸Œæœ›æœ€è¿‘é‚»è·ç¦» > margin
        loss_sep = torch.clamp(self.margin_phys - topk_dists, min=0).mean()
        
        return loss_sep
    
    def train_epoch(self, dataloader, optimizer, epoch):
        self.model.train()
        total_loss = 0
        loss_dict = defaultdict(float)
        n_batches = 0
        expected_batches = len(dataloader)
        
        # Warmup è°ƒåº¦ï¼šé€æ¸å¢åŠ ç‰©ç†åˆ†ç¦»æŸå¤±çš„æƒé‡
        if epoch <= self.warmup_epochs:
            alpha_phys = self.lambda_sep_phys * (epoch / self.warmup_epochs)
        else:
            alpha_phys = self.lambda_sep_phys
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Stage C]", total=expected_batches)
        
        for batch in pbar:
            X = batch['X'].to(self.device)
            
            optimizer.zero_grad()
            
            if self.use_amp:
                with autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):
                    outputs = self.model(X, stage='B')  # Stage C ç”¨ B çš„å‰å‘
                    X_hat = outputs['X_hat']
                    loss_rec = F.mse_loss(X_hat, X)
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šsep_phys åœ¨ autocast å¤–è®¡ç®—ï¼Œé¿å… float16 ä¸‹ exp/cdist æº¢å‡º
                # compute_physical_separation_loss å†…éƒ¨å·²å¼ºåˆ¶ float32
                loss_sep_phys = self.compute_physical_separation_loss()
                
                # ğŸ”§ å°† loss_rec è½¬ä¸º float32 åæ±‚å’Œ
                loss = loss_rec.float() + alpha_phys * loss_sep_phys
                
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                outputs = self.model(X, stage='B')
                X_hat = outputs['X_hat']
                loss_rec = F.mse_loss(X_hat, X)
                
                loss_sep_phys = self.compute_physical_separation_loss()
                
                loss = loss_rec + alpha_phys * loss_sep_phys
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
            
            total_loss += loss.item()
            loss_dict['rec'] += loss_rec.item()
            loss_dict['sep_phys'] += loss_sep_phys.item()
            n_batches += 1
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'rec': f'{loss_rec.item():.4f}',
                'sep_phys': f'{loss_sep_phys.item():.4f}',
                'batch': f'{n_batches}/{expected_batches}'
            })
        
        if n_batches != expected_batches:
            print(f"\n  âš  è­¦å‘Š: æœŸæœ› {expected_batches} æ‰¹æ¬¡, å®é™…è®­ç»ƒ {n_batches} æ‰¹æ¬¡")
        
        avg_loss = total_loss / n_batches
        avg_loss_dict = {k: v / n_batches for k, v in loss_dict.items()}
        
        return avg_loss, avg_loss_dict


# ============================================================================
# 5. Checkpointç®¡ç† (Colabæ–­ç‚¹ç»­è®­æ ¸å¿ƒ)
# ============================================================================

class CheckpointManager:
    """Checkpointç®¡ç†å™¨: æ”¯æŒæ–­ç‚¹ç»­è®­"""
    
    def __init__(self, output_dir: Path):
        self.checkpoint_dir = output_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.last_checkpoint_path = self.checkpoint_dir / 'last.pt'
    
    def save(self, 
             model: nn.Module,
             optimizer: torch.optim.Optimizer,
             stage: str,
             epoch: int,
             loss: float,
             config: Dict,
             scaler: Optional[GradScaler] = None):
        """ä¿å­˜å®Œæ•´æ¢å¤çŠ¶æ€"""
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'stage': stage,
            'epoch': epoch,
            'loss': loss,
            'config': config,
            'rng_state': save_rng_state(),
            'timestamp': datetime.datetime.now().isoformat()
        }
        
        if scaler is not None:
            checkpoint['scaler_state_dict'] = scaler.state_dict()
        
        # ä¿å­˜é˜¶æ®µcheckpoint
        stage_path = self.checkpoint_dir / f'stage{stage}_epoch_{epoch:03d}.pt'
        torch.save(checkpoint, stage_path)
        
        # ä¿å­˜last.pt (è¦†ç›–)
        torch.save(checkpoint, self.last_checkpoint_path)
        
        print(f"  âœ“ Checkpoint saved: {stage_path.name}")
    
    def load_last(self,
                  model: nn.Module,
                  optimizer: Optional[torch.optim.Optimizer] = None,
                  scaler: Optional[GradScaler] = None,
                  device: torch.device = torch.device('cpu')) -> Optional[Dict]:
        """åŠ è½½last.ptå¹¶æ¢å¤çŠ¶æ€"""
        if not self.last_checkpoint_path.exists():
            return None
        
        print(f"\nå‘ç°æ–­ç‚¹: {self.last_checkpoint_path}")
        checkpoint = safe_torch_load(self.last_checkpoint_path, map_location=device, weights_only=False)
        
        # æ¢å¤æ¨¡å‹
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # æ¢å¤ä¼˜åŒ–å™¨
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤scaler
        if scaler is not None and 'scaler_state_dict' in checkpoint:
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        # æ¢å¤éšæœºçŠ¶æ€
        if 'rng_state' in checkpoint:
            load_rng_state(checkpoint['rng_state'])
        
        print(f"  âœ“ ä» Stage {checkpoint['stage']} Epoch {checkpoint['epoch']} æ¢å¤")
        print(f"  âœ“ ä¿å­˜æ—¶é—´: {checkpoint['timestamp']}")
        
        return checkpoint
    
    def get_resume_info(self) -> Optional[Dict]:
        """è·å–æ¢å¤ä¿¡æ¯ (ä¸åŠ è½½æ¨¡å‹)"""
        if not self.last_checkpoint_path.exists():
            return None
        
        checkpoint = safe_torch_load(self.last_checkpoint_path, map_location='cpu', weights_only=False)
        return {
            'stage': checkpoint['stage'],
            'epoch': checkpoint['epoch'],
            'loss': checkpoint['loss'],
            'timestamp': checkpoint['timestamp']
        }


# ============================================================================
# 6. å¯¼å‡ºå™¨
# ============================================================================

class PrototypeExporter:
    """åŸå‹è§£é‡ŠåŒ…å¯¼å‡ºå™¨"""
    
    def __init__(self, 
                 model: DeepAABigModel,
                 scaler_path: str,
                 metadata_path: str,
                 output_dir: str,
                 device: torch.device):
        self.model = model.to(device)
        self.model.eval()
        self.device = device
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.scaler = ScalerAdapter(scaler_path)
        with open(metadata_path, 'r') as f:
            self.metadata = json.load(f)
        
        self.token_order = self.metadata['token_order']
        self.feature_names = self.metadata['feature_names']
    
    def export_prototype_tokens(self):
        K = self.model.n_prototypes
        X_proto_list = []
        
        for k in range(K):
            X_hat_k = self.model.generate_from_prototype(k)
            X_proto_list.append(X_hat_k.cpu().numpy())
        
        X_proto = np.stack(X_proto_list, axis=0)
        X_proto_phys = self.scaler.inverse_transform(X_proto)
        
        rows = []
        for k in range(K):
            row = {'prototype_id': k}
            for t_idx, token_name in enumerate(self.token_order):
                for f_idx, feat_name in enumerate(self.feature_names):
                    col_name = f"{token_name}_{feat_name}"
                    row[col_name] = X_proto_phys[k, t_idx, f_idx]
            rows.append(row)
        
        df = pd.DataFrame(rows)
        csv_path = self.output_dir / 'prototype_tokens.csv'
        df.to_csv(csv_path, index=False)
        
        print(f"âœ“ å¯¼å‡ºåŸå‹tokenç‰¹å¾: {csv_path}")
        return df, X_proto_phys
    
    def export_prototype_curves(self, X_proto_phys):
        """
        å¯¼å‡ºåŸå‹çš„æ—¶é—´åºåˆ—æ›²çº¿
        
        ä½¿ç”¨å‚…é‡Œå¶ç³»æ•°é‡å»ºå¹´å‘¨æœŸæ›²çº¿ï¼š
        - ä½¿ç”¨å‰ 10 ç»´ï¼š[a0, A1, u1, v1, A2, u2, v2, A3, u3, v3]
        - ç¬¬ 11 ç»´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰è¢«å¿½ç•¥ï¼ˆå‚è§ FOURIER_CURVE_DIMS å¸¸é‡ï¼‰
        
        æ›²çº¿å…¬å¼ï¼š
        y(t) = a0 + sum_{k=1}^{3} A_k * (u_k * cos(k*t) + v_k * sin(k*t))
        """
        K = self.model.n_prototypes
        n_days = 365
        t = np.linspace(0, 2*np.pi, n_days)
        
        curves_dir = self.output_dir / 'prototype_curves'
        curves_dir.mkdir(exist_ok=True)
        
        # æ£€æŸ¥ç»´åº¦å¹¶è­¦å‘Š
        feat_dim = X_proto_phys.shape[-1]
        if feat_dim > FOURIER_CURVE_DIMS:
            print(f"  âš  æ³¨æ„: ç‰¹å¾ç»´åº¦ä¸º {feat_dim}ï¼Œæ›²çº¿é‡å»ºä»…ä½¿ç”¨å‰ {FOURIER_CURVE_DIMS} ç»´")
            print(f"    å¿½ç•¥çš„ç»´åº¦: {list(range(FOURIER_CURVE_DIMS, feat_dim))}")
        
        for k in range(K):
            curves_k = {}
            
            for t_idx, token_name in enumerate(self.token_order):
                a0 = X_proto_phys[k, t_idx, 0]
                A1, u1, v1 = X_proto_phys[k, t_idx, 1:4]
                A2, u2, v2 = X_proto_phys[k, t_idx, 4:7]
                A3, u3, v3 = X_proto_phys[k, t_idx, 7:10]
                
                c1, s1 = A1 * u1, A1 * v1
                c2, s2 = A2 * u2, A2 * v2
                c3, s3 = A3 * u3, A3 * v3
                
                curve = (a0 + 
                        c1 * np.cos(t) + s1 * np.sin(t) +
                        c2 * np.cos(2*t) + s2 * np.sin(2*t) +
                        c3 * np.cos(3*t) + s3 * np.sin(3*t))
                
                curves_k[token_name] = curve
            
            np.savez(curves_dir / f'prototype_{k:03d}.npz', **curves_k, days=np.arange(n_days))
        
        # ä¿å­˜æ›²çº¿å…ƒæ•°æ®
        curve_meta = {
            'n_prototypes': K,
            'n_days': n_days,
            'fourier_dims_used': FOURIER_CURVE_DIMS,
            'total_feat_dim': feat_dim,
            'ignored_dims': list(range(FOURIER_CURVE_DIMS, feat_dim)) if feat_dim > FOURIER_CURVE_DIMS else [],
            'formula': 'y(t) = a0 + sum_{k=1}^{3} A_k * (u_k * cos(k*t) + v_k * sin(k*t))',
            'dim_mapping': {
                0: 'a0 (DC component)',
                1: 'A1 (1st harmonic amplitude)',
                2: 'u1 (1st harmonic cos coef)',
                3: 'v1 (1st harmonic sin coef)',
                4: 'A2 (2nd harmonic amplitude)',
                5: 'u2 (2nd harmonic cos coef)',
                6: 'v2 (2nd harmonic sin coef)',
                7: 'A3 (3rd harmonic amplitude)',
                8: 'u3 (3rd harmonic cos coef)',
                9: 'v3 (3rd harmonic sin coef)'
            }
        }
        with open(curves_dir / 'curve_metadata.json', 'w') as f:
            json.dump(curve_meta, f, indent=2)
        
        print(f"âœ“ å¯¼å‡ºåŸå‹æ›²çº¿: {curves_dir}")
        return curves_dir
    
    def export_physical_distance_matrix(self, X_proto_phys):
        K = self.model.n_prototypes
        X_flat = X_proto_phys.reshape(K, -1)
        
        from scipy.spatial.distance import pdist, squareform
        D_phys = squareform(pdist(X_flat, metric='euclidean'))
        
        npy_path = self.output_dir / 'D_phys.npy'
        np.save(npy_path, D_phys)
        
        csv_path = self.output_dir / 'D_phys.csv'
        pd.DataFrame(D_phys).to_csv(csv_path, index=False)
        
        print(f"âœ“ å¯¼å‡ºç‰©ç†è·ç¦»çŸ©é˜µ: {npy_path}")
        return D_phys
    
    def export_all(self):
        print("\n" + "="*80)
        print("å¯¼å‡ºåŸå‹è§£é‡ŠåŒ…")
        print("="*80 + "\n")
        
        df_tokens, X_proto_phys = self.export_prototype_tokens()
        curves_dir = self.export_prototype_curves(X_proto_phys)
        D_phys = self.export_physical_distance_matrix(X_proto_phys)
        
        print("\nâœ“ åŸå‹è§£é‡ŠåŒ…å¯¼å‡ºå®Œæˆ!\n")
        
        return {
            'tokens_df': df_tokens,
            'tokens_phys': X_proto_phys,
            'curves_dir': curves_dir,
            'D_phys': D_phys
        }


class DistillationMLPStudent(nn.Module):
    """è’¸é¦å­¦ç”Ÿæ¨¡å‹"""
    
    def __init__(self,
                 input_dim=121,
                 hidden_dims=[256, 128],
                 output_dim=50,
                 activation='tanh',
                 temperature=1.0):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.activation_name = activation
        self.temperature = temperature
        
        layers = []
        in_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(in_dim, hidden_dim))
            if activation == 'tanh':
                layers.append(nn.Tanh())
            elif activation == 'relu':
                layers.append(nn.ReLU())
            in_dim = hidden_dim
        
        layers.append(nn.Linear(in_dim, output_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        logits = self.network(x)
        probs = F.softmax(logits / self.temperature, dim=-1)
        return probs, logits


class DistillationTrainer:
    """è’¸é¦è®­ç»ƒå™¨ - ä¿®å¤KLæ–¹å‘"""
    
    def __init__(self,
                 teacher_model: DeepAABigModel,
                 student_model: DistillationMLPStudent,
                 device: torch.device,
                 lambda_entropy: float = 0.001,
                 use_amp: bool = True):
        self.teacher = teacher_model.to(device).eval()
        self.student = student_model.to(device)
        self.device = device
        self.lambda_entropy = lambda_entropy
        self.use_amp = use_amp and device.type == 'cuda'
        
        # å§‹ç»ˆåˆ›å»º scaler å±æ€§ï¼Œé¿å… AttributeError
        self.scaler = GradScaler() if self.use_amp else None
    
    @torch.no_grad()
    def generate_soft_labels(self, X):
        if self.use_amp:
            with autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):
                outputs = self.teacher(X, stage='B')
        else:
            outputs = self.teacher(X, stage='B')
        return outputs['w']
    
    def train_epoch(self, dataloader, optimizer, epoch, max_batches: Optional[int] = None):
        self.student.train()
        total_loss = 0
        total_kl = 0
        total_entropy = 0
        n_batches = 0
        
        pbar = tqdm(dataloader, desc=f"Distill Epoch {epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            if max_batches is not None and batch_idx >= max_batches:
                break
            
            X = batch['X'].to(self.device)
            B = X.size(0)
            X_flat = X.view(B, -1)
            
            w_teacher = self.generate_soft_labels(X)
            
            optimizer.zero_grad()
            
            # ğŸ”§ AMPç¨³å®šæ€§ä¿®å¤ï¼šeps ä» 1e-8 æå‡åˆ° 1e-6ï¼Œé€‚é… float16 ç²¾åº¦
            eps = 1e-6
            
            if self.use_amp:
                with autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):
                    w_student, _ = self.student(X_flat)
                    
                    # ä¿®å¤KLæ•£åº¦è®¡ç®—æ–¹å‘
                    # KL(P_teacher || P_student) = sum(P_teacher * log(P_teacher / P_student))
                    # PyTorchçš„kl_divæœŸæœ›è¾“å…¥æ˜¯logæ¦‚ç‡ï¼Œç›®æ ‡æ˜¯æ¦‚ç‡
                    # kl_div(log_student, teacher) = teacher * (log(teacher) - log_student)
                    
                    # ğŸ”§ æ­£ç¡®å†™æ³•ï¼šstudent å– logï¼Œteacher ä½œä¸ºç›®æ ‡
                    # eps ä» 1e-8 æå‡åˆ° 1e-6ï¼Œfloat16 ä¸‹ 1e-8 ä¼šå˜æˆ 0
                    loss_kl = F.kl_div(
                        (w_student + eps).log(),  # log(student)
                        w_teacher,                 # teacher (æ¦‚ç‡)
                        reduction='batchmean',
                        log_target=False           # teacher ä¸æ˜¯ log å½¢å¼
                    )
                    
                    # ç†µæ­£åˆ™åŒ–ï¼šé¼“åŠ±å­¦ç”Ÿè¾“å‡ºæ›´é›†ä¸­
                    entropy = -(w_student * (w_student + eps).log()).sum(dim=1).mean()
                    
                    # æœ€å°åŒ– KLï¼ŒåŒæ—¶æœ€å°åŒ–ç†µï¼ˆæ›´ç¨€ç–ï¼‰
                    loss = loss_kl + self.lambda_entropy * entropy
                
                self.scaler.scale(loss).backward()
                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                w_student, _ = self.student(X_flat)
                
                loss_kl = F.kl_div(
                    (w_student + eps).log(),
                    w_teacher,
                    reduction='batchmean',
                    log_target=False
                )
                
                entropy = -(w_student * (w_student + eps).log()).sum(dim=1).mean()
                loss = loss_kl + self.lambda_entropy * entropy
                
                loss.backward()
                optimizer.step()
            
            total_loss += loss.item()
            total_kl += loss_kl.item()
            total_entropy += entropy.item()
            n_batches += 1
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'kl': f'{loss_kl.item():.4f}',
                'entropy': f'{entropy.item():.4f}'
            })
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if n_batches == 0:
            warnings.warn("è’¸é¦è®­ç»ƒæ²¡æœ‰å¤„ç†ä»»ä½•æ‰¹æ¬¡ï¼Œè¯·æ£€æŸ¥ max_batches å‚æ•°")
            return 0.0, {'kl': 0.0, 'entropy': 0.0}
        
        avg_loss = total_loss / n_batches
        avg_kl = total_kl / n_batches
        avg_entropy = total_entropy / n_batches
        
        return avg_loss, {'kl': avg_kl, 'entropy': avg_entropy}


class DistillationExporter:
    """è’¸é¦æ¨¡å‹å¯¼å‡ºå™¨ (GEEæ ¼å¼)"""
    
    def __init__(self, student_model: DistillationMLPStudent, output_dir: str, metadata: Dict):
        self.student = student_model
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.metadata = metadata
    
    def export_weights(self):
        weights_dict = {}
        layer_idx = 0
        
        for name, param in self.student.named_parameters():
            if 'weight' in name:
                weights_dict[f'layer_{layer_idx}_weight'] = param.detach().cpu().numpy().tolist()
            elif 'bias' in name:
                weights_dict[f'layer_{layer_idx}_bias'] = param.detach().cpu().numpy().tolist()
                layer_idx += 1
        
        json_path = self.output_dir / 'distill_weights.json'
        with open(json_path, 'w') as f:
            json.dump(weights_dict, f, indent=2)
        
        print(f"âœ“ å¯¼å‡ºè’¸é¦æƒé‡: {json_path}")
        return json_path
    
    def export_config(self):
        config = {
            'input_dim': self.student.input_dim,
            'hidden_dims': self.student.hidden_dims,
            'output_dim': self.student.output_dim,
            'activation': self.student.activation_name,
            'temperature': self.student.temperature,
            'token_order': self.metadata['token_order'],
            'feature_names': self.metadata['feature_names'],
            'flatten_order': 'row-major'
        }
        
        json_path = self.output_dir / 'distill_config.json'
        with open(json_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ“ å¯¼å‡ºè’¸é¦é…ç½®: {json_path}")
        return json_path
    
    def export_all(self):
        print("\n" + "="*80)
        print("å¯¼å‡ºè’¸é¦æ¨æ–­åŒ… (GEEéƒ¨ç½²ç”¨)")
        print("="*80 + "\n")
        
        weights_path = self.export_weights()
        config_path = self.export_config()
        
        print("\nâœ“ è’¸é¦æ¨æ–­åŒ…å¯¼å‡ºå®Œæˆ!\n")
        
        return {'weights_path': weights_path, 'config_path': config_path}


# ============================================================================
# 7. è¯Šæ–­ä¸å¯è§†åŒ–æ¨¡å— (é›†æˆç‰ˆ - åœ¨è®­ç»ƒä¸­è¿è¡Œ)
# ============================================================================

class PrototypeDiagnostics:
    """åŸå‹è¯Šæ–­å·¥å…· (é‡‡æ ·ä¼˜åŒ–ç‰ˆ - è§£å†³å¤§æ•°æ®é›†æ€§èƒ½é—®é¢˜)"""
    
    def __init__(self, 
                 model: DeepAABigModel,
                 device: torch.device,
                 output_dir: str,
                 max_samples: int = 50000):
        """
        å‚æ•°:
            max_samples: è¯Šæ–­æ—¶æœ€å¤§é‡‡æ ·æ•°ï¼ˆé»˜è®¤50Kï¼Œé¿å…å…¨é‡éå†7.7Mæ ·æœ¬ï¼‰
        """
        self.model = model.to(device).eval()
        self.device = device
        self.output_dir = Path(output_dir)
        self.diag_dir = self.output_dir / 'diagnostics'
        self.diag_dir.mkdir(parents=True, exist_ok=True)
        self.max_samples = max_samples
    
    def compute_prototype_usage(self, dataloader, save_plots=True):
        """
        è®¡ç®—åŸå‹ä½¿ç”¨ç»Ÿè®¡ (é‡‡æ ·ä¼˜åŒ–ç‰ˆ)
        
        âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šåªé‡‡æ ·å‰ max_samples ä¸ªæ ·æœ¬ï¼Œé¿å…éå†å…¨éƒ¨æ•°æ®
        """
        print("\n" + "="*80)
        print("è¯Šæ–­1: åŸå‹ä½¿ç”¨åˆ†æ")
        print("="*80 + "\n")
        
        K = self.model.n_prototypes
        total_samples = 0
        weight_sum = np.zeros(K)
        weight_sq_sum = np.zeros(K)
        weight_max = np.zeros(K)
        assignment_counts = np.zeros(K)
        sampled_weights = []
        
        print(f"âš¡ é‡‡æ ·æ¨¡å¼ï¼šæœ€å¤šå¤„ç† {self.max_samples:,} ä¸ªæ ·æœ¬ï¼ˆé¿å…å…¨é‡éå†ï¼‰")
        
        self.model.eval()
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(dataloader, desc="é‡‡æ ·åˆ†æ")):
                # æå‰ç»ˆæ­¢ï¼šè¾¾åˆ°é‡‡æ ·ä¸Šé™
                if total_samples >= self.max_samples:
                    print(f"âœ“ å·²é‡‡æ · {total_samples:,} ä¸ªæ ·æœ¬ï¼Œåœæ­¢éå†")
                    break
                
                X = batch['X'].to(self.device)
                B = X.size(0)
                
                # å¦‚æœå½“å‰batchä¼šè¶…è¿‡ä¸Šé™ï¼Œåªå–éƒ¨åˆ†
                if total_samples + B > self.max_samples:
                    remaining = self.max_samples - total_samples
                    X = X[:remaining]
                    B = remaining
                
                outputs = self.model(X, stage='B')
                w = outputs['w'].cpu().numpy()  # (B, K)
                
                # æµå¼ç»Ÿè®¡é‡è®¡ç®—
                weight_sum += w.sum(axis=0)
                weight_sq_sum += (w ** 2).sum(axis=0)
                weight_max = np.maximum(weight_max, w.max(axis=0))
                
                # ç¡¬åˆ†é…
                assignments = w.argmax(axis=1)
                for k in assignments:
                    assignment_counts[k] += 1
                
                total_samples += B
                
                # ä¿å­˜éƒ¨åˆ†æƒé‡ç”¨äºç»˜å›¾
                if len(sampled_weights) < 100:  # æœ€å¤šä¿å­˜100ä¸ªbatchç”¨äºå¯è§†åŒ–
                    sampled_weights.append(w[:min(100, B)])
        
        # åˆå¹¶é‡‡æ ·çš„æƒé‡ï¼ˆç”¨äºç»˜å›¾ï¼‰
        sampled_weights = np.vstack(sampled_weights) if sampled_weights else None
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_weights = weight_sum / total_samples
        weight_std = np.sqrt(weight_sq_sum / total_samples - mean_weights ** 2)
        usage_rate = assignment_counts / total_samples
        
        # è¯†åˆ« dead prototypes (ä½¿ç”¨ç‡ < 0.1%)
        threshold = 0.001
        dead_mask = usage_rate < threshold
        dead_prototypes = np.where(dead_mask)[0].tolist()
        
        usage_stats = {
            'counts': assignment_counts,
            'mean_weights': mean_weights,
            'weight_std': weight_std,
            'max_weights': weight_max,
            'usage_rate': usage_rate,
            'dead_prototypes': dead_prototypes,
            'total_samples': total_samples,
            'sampled_weights': sampled_weights,
            'is_sampled': total_samples < len(dataloader.dataset) if hasattr(dataloader, 'dataset') else True
        }
        
        # æ‰“å°æŠ¥å‘Š
        print(f"\nåˆ†ææ ·æœ¬æ•°: {total_samples:,} {'[é‡‡æ ·]' if usage_stats['is_sampled'] else '[å…¨é‡]'}")
        print(f"åŸå‹æ•°: {K}")
        print(f"Dead Prototypes (ä½¿ç”¨ç‡ < {threshold*100:.1f}%): {len(dead_prototypes)}")
        if dead_prototypes:
            print(f"  IDåˆ—è¡¨: {dead_prototypes}")
        print(f"\nä½¿ç”¨ç‡ç»Ÿè®¡:")
        print(f"  æœ€å°: {usage_rate.min()*100:.2f}%")
        print(f"  æœ€å¤§: {usage_rate.max()*100:.2f}%")
        print(f"  å¹³å‡: {usage_rate.mean()*100:.2f}%")
        print(f"  æ ‡å‡†å·®: {usage_rate.std()*100:.2f}%")
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡
        stats_df = pd.DataFrame({
            'prototype_id': np.arange(K),
            'assignment_count': assignment_counts,
            'usage_rate': usage_rate,
            'mean_weight': mean_weights,
            'max_weight': weight_max,
            'is_dead': dead_mask
        })
        stats_df.to_csv(self.diag_dir / 'prototype_usage_stats.csv', index=False)
        print(f"\nâœ“ è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜: {self.diag_dir / 'prototype_usage_stats.csv'}")
        
        # ç»˜å›¾
        if save_plots:
            self._plot_usage_histogram(usage_stats)
        
        return usage_stats
    
    def _plot_usage_histogram(self, usage_stats):
        """ç»˜åˆ¶åŸå‹ä½¿ç”¨ç›´æ–¹å›¾"""
        K = len(usage_stats['usage_rate'])
        usage_rate = usage_stats['usage_rate'] * 100
        dead_prototypes = usage_stats['dead_prototypes']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ä½¿ç”¨ç‡ç›´æ–¹å›¾
        ax = axes[0, 0]
        colors = ['red' if k in dead_prototypes else 'steelblue' for k in range(K)]
        ax.bar(np.arange(K), usage_rate, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)
        ax.axhline(0.1, color='red', linestyle='--', linewidth=2, label='Dead Threshold (0.1%)')
        ax.set_xlabel('Prototype ID', fontsize=12)
        ax.set_ylabel('Usage Rate (%)', fontsize=12)
        ax.set_title(f'Prototype Usage Distribution\n({len(dead_prototypes)} dead prototypes)', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. ä½¿ç”¨ç‡åˆ†å¸ƒç›´æ–¹å›¾
        ax = axes[0, 1]
        ax.hist(usage_rate, bins=30, color='steelblue', alpha=0.7, edgecolor='black')
        ax.axvline(usage_rate.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {usage_rate.mean():.2f}%')
        ax.set_xlabel('Usage Rate (%)', fontsize=12)
        ax.set_ylabel('Count', fontsize=12)
        ax.set_title('Usage Rate Distribution', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. å¹³å‡æƒé‡åˆ†å¸ƒ
        ax = axes[1, 0]
        mean_weights = usage_stats['mean_weights']
        colors = ['red' if k in dead_prototypes else 'steelblue' for k in range(K)]
        ax.bar(np.arange(K), mean_weights, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)
        ax.set_xlabel('Prototype ID', fontsize=12)
        ax.set_ylabel('Mean Weight', fontsize=12)
        ax.set_title('Mean Prototype Weights', fontsize=14)
        ax.grid(True, alpha=0.3)
        
        # 4. Top-10 å’Œ Bottom-10
        ax = axes[1, 1]
        sorted_idx = np.argsort(usage_rate)
        top10_idx = sorted_idx[-10:][::-1]
        bottom10_idx = sorted_idx[:10]
        
        y_pos = np.arange(20)
        labels = [f'P{i}' for i in top10_idx] + [f'P{i}' for i in bottom10_idx]
        values = np.concatenate([usage_rate[top10_idx], usage_rate[bottom10_idx]])
        colors_top = ['green'] * 10 + ['red'] * 10
        
        ax.barh(y_pos, values, color=colors_top, alpha=0.7, edgecolor='black')
        ax.set_yticks(y_pos)
        ax.set_yticklabels(labels, fontsize=10)
        ax.axvline(usage_rate.mean(), color='blue', linestyle='--', linewidth=2, label='Mean')
        ax.set_xlabel('Usage Rate (%)', fontsize=12)
        ax.set_title('Top-10 & Bottom-10 Prototypes', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        save_path = self.diag_dir / 'prototype_usage_histogram.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ ä½¿ç”¨ç›´æ–¹å›¾å·²ä¿å­˜: {save_path}")
    
    def compute_aez_prototype_heatmap(self, dataloader, save_plots=True):
        """
        è®¡ç®— AEZ Ã— Prototype çƒ­åŠ›å›¾ (é‡‡æ ·ä¼˜åŒ–ç‰ˆ)
        
        âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šåªé‡‡æ ·å‰ max_samples ä¸ªæ ·æœ¬
        """
        print("\n" + "="*80)
        print("è¯Šæ–­2: AEZ Ã— Prototype çƒ­åŠ›å›¾")
        print("="*80 + "\n")
        
        K = self.model.n_prototypes
        total_samples = 0
        
        # ä½¿ç”¨å­—å…¸è¿›è¡Œæµå¼èšåˆ
        aez_weight_sum = defaultdict(lambda: np.zeros(K))
        aez_count = defaultdict(int)
        
        print(f"âš¡ é‡‡æ ·æ¨¡å¼ï¼šæœ€å¤šå¤„ç† {self.max_samples:,} ä¸ªæ ·æœ¬")
        
        self.model.eval()
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="é‡‡æ ·åˆ†æAEZ"):
                # æå‰ç»ˆæ­¢
                if total_samples >= self.max_samples:
                    print(f"âœ“ å·²é‡‡æ · {total_samples:,} ä¸ªæ ·æœ¬ï¼Œåœæ­¢éå†")
                    break
                
                X = batch['X'].to(self.device)
                
                if 'aez' not in batch:
                    print("  âš  æ•°æ®é›†ä¸åŒ…å«AEZä¿¡æ¯ï¼Œè·³è¿‡çƒ­åŠ›å›¾ç”Ÿæˆ")
                    return None
                
                aez = batch['aez'].cpu().numpy()
                B = len(aez)
                
                # é™åˆ¶batchå¤§å°
                if total_samples + B > self.max_samples:
                    remaining = self.max_samples - total_samples
                    X = X[:remaining]
                    aez = aez[:remaining]
                    B = remaining
                
                outputs = self.model(X, stage='B')
                w = outputs['w'].cpu().numpy()  # (B, K)
                
                # æµå¼èšåˆï¼šæŒ‰ AEZ ç´¯åŠ æƒé‡
                for i, aez_id in enumerate(aez):
                    aez_weight_sum[aez_id] += w[i]
                    aez_count[aez_id] += 1
                
                total_samples += B
        
        # è®¡ç®—æ¯ä¸ª AEZ çš„å¹³å‡æƒé‡
        unique_aez = sorted(aez_weight_sum.keys())
        n_aez = len(unique_aez)
        
        heatmap = np.zeros((n_aez, K))
        for i, aez_id in enumerate(unique_aez):
            heatmap[i, :] = aez_weight_sum[aez_id] / aez_count[aez_id]
        
        unique_aez = np.array(unique_aez)
        
        # å½’ä¸€åŒ–æ¯è¡Œ
        heatmap_norm = heatmap / heatmap.sum(axis=1, keepdims=True)
        
        # ä¿å­˜æ•°æ®
        heatmap_df = pd.DataFrame(
            heatmap_norm,
            index=[f'AEZ_{int(aez_id)}' for aez_id in unique_aez],
            columns=[f'P{k}' for k in range(K)]
        )
        heatmap_df.to_csv(self.diag_dir / 'aez_prototype_heatmap.csv')
        print(f"\nâœ“ çƒ­åŠ›å›¾æ•°æ®å·²ä¿å­˜: {self.diag_dir / 'aez_prototype_heatmap.csv'}")
        print(f"  åˆ†ææ ·æœ¬æ•°: {total_samples:,} [é‡‡æ ·]")
        
        if save_plots:
            self._plot_aez_prototype_heatmap(heatmap_norm, unique_aez)
        
        # æ‰¾æ¯ä¸ªAEZçš„Top-5åŸå‹
        top5_per_aez = {}
        for i, aez_id in enumerate(unique_aez):
            top5_idx = np.argsort(heatmap_norm[i])[-5:][::-1]
            top5_weights = heatmap_norm[i, top5_idx]
            top5_per_aez[int(aez_id)] = list(zip(top5_idx.tolist(), top5_weights.tolist()))
        
        print(f"\næ¯ä¸ªAEZçš„Top-5åŸå‹:")
        for aez_id in sorted(top5_per_aez.keys())[:10]:
            top5 = top5_per_aez[aez_id]
            top5_str = ", ".join([f"P{k}({w*100:.1f}%)" for k, w in top5])
            print(f"  AEZ {aez_id}: {top5_str}")
        
        if len(unique_aez) > 10:
            print(f"  ... (å…± {n_aez} ä¸ªAEZ)")
        
        return {
            'heatmap': heatmap_norm,
            'unique_aez': unique_aez,
            'top5_per_aez': top5_per_aez
        }
    
    def _plot_aez_prototype_heatmap(self, heatmap, unique_aez):
        """ç»˜åˆ¶AEZÃ—åŸå‹çƒ­åŠ›å›¾"""
        n_aez, K = heatmap.shape
        
        max_display = 50
        if n_aez > max_display:
            print(f"  âš  AEZæ•°é‡è¿‡å¤š ({n_aez})ï¼Œåªæ˜¾ç¤ºå‰ {max_display} ä¸ª")
            heatmap_plot = heatmap[:max_display]
            aez_labels = [f'AEZ_{int(aez)}' for aez in unique_aez[:max_display]]
        else:
            heatmap_plot = heatmap
            aez_labels = [f'AEZ_{int(aez)}' for aez in unique_aez]
        
        fig, ax = plt.subplots(figsize=(max(12, K*0.3), max(8, len(aez_labels)*0.2)))
        
        im = ax.imshow(heatmap_plot, aspect='auto', cmap='YlOrRd', interpolation='nearest')
        
        ax.set_xticks(np.arange(K))
        ax.set_yticks(np.arange(len(aez_labels)))
        ax.set_xticklabels([f'P{k}' for k in range(K)], rotation=90, fontsize=8)
        ax.set_yticklabels(aez_labels, fontsize=8)
        
        ax.set_xlabel('Prototype ID', fontsize=12)
        ax.set_ylabel('AEZ', fontsize=12)
        ax.set_title('AEZ Ã— Prototype Usage Heatmap\n(normalized by row)', fontsize=14)
        
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('Weight', fontsize=12)
        
        plt.tight_layout()
        save_path = self.diag_dir / 'aez_prototype_heatmap.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ AEZçƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")


class DistillationDiagnostics:
    """è’¸é¦ä¸€è‡´æ€§è¯Šæ–­"""
    
    def __init__(self,
                 teacher_model: DeepAABigModel,
                 student_model: DistillationMLPStudent,
                 device: torch.device,
                 output_dir: str):
        self.teacher = teacher_model.to(device).eval()
        self.student = student_model.to(device).eval()
        self.device = device
        self.output_dir = Path(output_dir)
        self.diag_dir = self.output_dir / 'diagnostics'
        self.diag_dir.mkdir(parents=True, exist_ok=True)
    
    def compute_distillation_consistency(self, dataloader, save_plots=True):
        """è®¡ç®—teacher vs studentä¸€è‡´æ€§"""
        print("\n" + "="*80)
        print("è¯Šæ–­3: Teacher-Student ä¸€è‡´æ€§åˆ†æ")
        print("="*80 + "\n")
        
        kl_divs = []
        top1_matches = []
        top5_recalls = []
        correlations = []
        
        teacher_weights_all = []
        student_weights_all = []
        
        self.teacher.eval()
        self.student.eval()
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="è®¡ç®—ä¸€è‡´æ€§"):
                X = batch['X'].to(self.device)
                B = X.size(0)
                X_flat = X.view(B, -1)
                
                # Teacheré¢„æµ‹
                outputs_t = self.teacher(X, stage='B')
                w_teacher = outputs_t['w']
                
                # Studenté¢„æµ‹
                w_student, _ = self.student(X_flat)
                
                w_t = w_teacher.cpu().numpy()
                w_s = w_student.cpu().numpy()
                
                teacher_weights_all.append(w_t)
                student_weights_all.append(w_s)
                
                # 1. KLæ•£åº¦
                # ğŸ”§ eps æå‡åˆ° 1e-6ï¼Œä¸è®­ç»ƒä»£ç ä¿æŒä¸€è‡´
                eps = 1e-6
                kl_per_sample = (w_t * np.log((w_t + eps) / (w_s + eps))).sum(axis=1)
                kl_divs.extend(kl_per_sample.tolist())
                
                # 2. Top-1ä¸€è‡´æ€§
                top1_t = w_t.argmax(axis=1)
                top1_s = w_s.argmax(axis=1)
                matches = (top1_t == top1_s).astype(float)
                top1_matches.extend(matches.tolist())
                
                # 3. Top-5 Recall
                top5_s = np.argsort(w_s, axis=1)[:, -5:]
                recalls = np.array([top1_t[i] in top5_s[i] for i in range(B)]).astype(float)
                top5_recalls.extend(recalls.tolist())
                
                # 4. æƒé‡ç›¸å…³æ€§
                for i in range(B):
                    corr = np.corrcoef(w_t[i], w_s[i])[0, 1]
                    correlations.append(corr)
        
        teacher_weights_all = np.vstack(teacher_weights_all)
        student_weights_all = np.vstack(student_weights_all)
        
        kl_divs = np.array(kl_divs)
        top1_matches = np.array(top1_matches)
        top5_recalls = np.array(top5_recalls)
        correlations = np.array(correlations)
        
        consistency_stats = {
            'kl_mean': kl_divs.mean(),
            'kl_std': kl_divs.std(),
            'kl_median': np.median(kl_divs),
            'top1_accuracy': top1_matches.mean(),
            'top5_recall': top5_recalls.mean(),
            'correlation_mean': correlations.mean(),
            'correlation_std': correlations.std(),
            'n_samples': len(kl_divs)
        }
        
        # æ‰“å°æŠ¥å‘Š
        print(f"æ ·æœ¬æ•°: {consistency_stats['n_samples']:,}")
        print(f"\nKLæ•£åº¦ (Teacher || Student):")
        print(f"  Mean: {consistency_stats['kl_mean']:.4f}")
        print(f"  Std:  {consistency_stats['kl_std']:.4f}")
        print(f"  Median: {consistency_stats['kl_median']:.4f}")
        print(f"\nTop-1 ä¸€è‡´ç‡: {consistency_stats['top1_accuracy']*100:.2f}%")
        print(f"Top-5 Recall: {consistency_stats['top5_recall']*100:.2f}%")
        print(f"\næƒé‡ç›¸å…³æ€§ (Pearson):")
        print(f"  Mean: {consistency_stats['correlation_mean']:.4f}")
        print(f"  Std:  {consistency_stats['correlation_std']:.4f}")
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡
        details_df = pd.DataFrame({
            'kl_divergence': kl_divs,
            'top1_match': top1_matches,
            'top5_recall': top5_recalls,
            'correlation': correlations
        })
        details_df.to_csv(self.diag_dir / 'distillation_consistency_details.csv', index=False)
        print(f"\nâœ“ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {self.diag_dir / 'distillation_consistency_details.csv'}")
        
        summary_df = pd.DataFrame([consistency_stats])
        summary_df.to_csv(self.diag_dir / 'distillation_consistency_summary.csv', index=False)
        
        if save_plots:
            self._plot_consistency_distributions(consistency_stats, kl_divs, correlations)
            self._plot_weight_scatter(teacher_weights_all, student_weights_all)
        
        return consistency_stats
    
    def _plot_consistency_distributions(self, stats, kl_divs, correlations):
        """ç»˜åˆ¶ä¸€è‡´æ€§åˆ†å¸ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. KLæ•£åº¦åˆ†å¸ƒ
        ax = axes[0, 0]
        ax.hist(kl_divs, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
        ax.axvline(stats['kl_mean'], color='red', linestyle='--', linewidth=2, 
                   label=f"Mean: {stats['kl_mean']:.4f}")
        ax.axvline(stats['kl_median'], color='green', linestyle='--', linewidth=2,
                   label=f"Median: {stats['kl_median']:.4f}")
        ax.set_xlabel('KL Divergence', fontsize=12)
        ax.set_ylabel('Count', fontsize=12)
        ax.set_title('KL Divergence Distribution\n(Teacher || Student)', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. ç›¸å…³æ€§åˆ†å¸ƒ
        ax = axes[0, 1]
        ax.hist(correlations, bins=50, color='coral', alpha=0.7, edgecolor='black')
        ax.axvline(stats['correlation_mean'], color='red', linestyle='--', linewidth=2,
                   label=f"Mean: {stats['correlation_mean']:.4f}")
        ax.set_xlabel('Pearson Correlation', fontsize=12)
        ax.set_ylabel('Count', fontsize=12)
        ax.set_title('Weight Correlation Distribution', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. ä¸€è‡´æ€§æŒ‡æ ‡æŸ±çŠ¶å›¾
        ax = axes[1, 0]
        metrics = ['Top-1\nAccuracy', 'Top-5\nRecall', 'Mean\nCorrelation']
        values = [
            stats['top1_accuracy'] * 100,
            stats['top5_recall'] * 100,
            (stats['correlation_mean'] + 1) * 50
        ]
        colors = ['green', 'blue', 'purple']
        bars = ax.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')
        
        for bar, val, raw_val in zip(bars, values, [stats['top1_accuracy'], 
                                                      stats['top5_recall'],
                                                      stats['correlation_mean']]):
            height = bar.get_height()
            if raw_val <= 1:
                label = f'{raw_val*100:.1f}%'
            else:
                label = f'{raw_val:.3f}'
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   label, ha='center', va='bottom', fontsize=11, fontweight='bold')
        
        ax.set_ylabel('Value (%)', fontsize=12)
        ax.set_title('Consistency Metrics Summary', fontsize=14)
        ax.set_ylim([0, 105])
        ax.grid(True, alpha=0.3, axis='y')
        
        # 4. KL vs Correlation æ•£ç‚¹å›¾
        ax = axes[1, 1]
        scatter = ax.scatter(correlations, kl_divs, alpha=0.3, s=10, c=kl_divs, cmap='coolwarm')
        ax.set_xlabel('Pearson Correlation', fontsize=12)
        ax.set_ylabel('KL Divergence', fontsize=12)
        ax.set_title('KL vs Correlation', fontsize=14)
        ax.grid(True, alpha=0.3)
        plt.colorbar(scatter, ax=ax, label='KL Divergence')
        
        plt.tight_layout()
        save_path = self.diag_dir / 'distillation_consistency_distributions.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ ä¸€è‡´æ€§åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
    
    def _plot_weight_scatter(self, w_teacher, w_student, n_samples=5000):
        """ç»˜åˆ¶æƒé‡æ•£ç‚¹å›¾"""
        if len(w_teacher) > n_samples:
            idx = np.random.choice(len(w_teacher), n_samples, replace=False)
            w_t = w_teacher[idx].flatten()
            w_s = w_student[idx].flatten()
        else:
            w_t = w_teacher.flatten()
            w_s = w_student.flatten()
        
        fig, ax = plt.subplots(figsize=(10, 10))
        
        h = ax.hist2d(w_t, w_s, bins=100, cmap='Blues', cmin=1)
        
        max_val = max(w_t.max(), w_s.max())
        ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Agreement')
        
        ax.set_xlabel('Teacher Weights', fontsize=12)
        ax.set_ylabel('Student Weights', fontsize=12)
        ax.set_title(f'Teacher vs Student Weight Scatter\n(sampled {len(w_t):,} points)', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.colorbar(h[3], ax=ax, label='Density')
        plt.tight_layout()
        
        save_path = self.diag_dir / 'weight_scatter.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ æƒé‡æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")

# ============================================================================
# 8 ä¿®æ”¹ DeepAATrainingPipelineColab ç±»ä¸­çš„è®­ç»ƒæ–¹æ³•
# ============================================================================

class DeepAATrainingPipelineColab:
    """DeepAAå®Œæ•´è®­ç»ƒç®¡é“ (ä¿®æ”¹ç‰ˆï¼šé›†æˆè¯Šæ–­)"""
    
    def __init__(self,
                 data_dir: str,
                 output_dir: str,
                 n_prototypes: int = 50,
                 d_model: int = 256,
                 n_heads: int = 8,
                 n_encoder_layers: int = 4,
                 entmax_n_iter: int = 15,
                 use_amp: bool = True,
                 seed: int = 42):
        
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.n_prototypes = n_prototypes
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_encoder_layers = n_encoder_layers
        self.entmax_n_iter = entmax_n_iter
        self.use_amp = use_amp
        self.seed = seed
        self.device = get_device()
        
        # è®¾ç½®éšæœºç§å­
        set_seed(seed)
        
        print(f"\nä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"éšæœºç§å­: {seed}\n")
        
        # åŠ è½½å…ƒæ•°æ®
        with open(self.data_dir / 'metadata.json', 'r') as f:
            self.metadata = json.load(f)
        
        # æ¨¡å‹
        self.model = None
        self.student_model = None
        
        # è®­ç»ƒæ—¥å¿—è®°å½•å™¨
        self.logger = TrainingLogger(self.output_dir)
        
        # Checkpointç®¡ç†å™¨
        self.checkpoint_mgr = CheckpointManager(self.output_dir)
        
        # è¯Šæ–­é…ç½®
        self.diagnostics_enabled = True  # æ§åˆ¶æ˜¯å¦å¯ç”¨è¯Šæ–­
        self.diagnostics_interval = 100  # æ¯Nä¸ªepochè¯Šæ–­ä¸€æ¬¡
        
        # ä¿å­˜è¿è¡Œé…ç½®
        self._save_run_config()
    
    def _save_run_config(self):
        """ä¿å­˜è¿è¡Œé…ç½®å¿«ç…§"""
        config = {
            'timestamp': datetime.datetime.now().isoformat(),
            'seed': self.seed,
            'device': str(self.device),
            'n_prototypes': self.n_prototypes,
            'd_model': self.d_model,
            'n_heads': self.n_heads,
            'n_encoder_layers': self.n_encoder_layers,
            'entmax_n_iter': self.entmax_n_iter,
            'use_amp': self.use_amp,
            'data_dir': str(self.data_dir),
            'output_dir': str(self.output_dir),
            'metadata': self.metadata,
            'pytorch_version': torch.__version__,
            'numpy_version': np.__version__,
            'pandas_version': pd.__version__
        }
        
        self.logger.save_config(config)
    
    def load_data(self, batch_size=512, use_stratified=True, num_workers=2, n_batches_per_epoch=None):
        """åŠ è½½æ•°æ®"""
        print("åŠ è½½è®­ç»ƒæ•°æ®...")
        
        train_data = np.load(self.data_dir / 'train_data.npz')
        X_train = train_data['X']
        aez_train = train_data['aez'] if 'aez' in train_data and len(train_data['aez']) > 0 else None
        ids_train = train_data['ids'] if 'ids' in train_data and len(train_data['ids']) > 0 else None
        
        print(f"  è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬")
        
        train_dataset = DeepAATokenDataset(X_train, aez_train, ids_train)
        
        if use_stratified and aez_train is not None:
            print("  ä½¿ç”¨åˆ†å±‚é‡‡æ ·")
            sampler = StratifiedBatchSampler(
                aez_train,
                batch_size=batch_size,
                shuffle=True,
                n_batches_per_epoch=n_batches_per_epoch
            )
            train_loader = DataLoader(
                train_dataset,
                batch_sampler=sampler,
                num_workers=num_workers
            )
        else:
            print("  ä½¿ç”¨éšæœºé‡‡æ ·")
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers
            )
        
        return train_loader
    
    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("\næ„å»ºDeepAA Big Model...")
        
        self.model = DeepAABigModel(
            n_prototypes=self.n_prototypes,
            d_model=self.d_model,
            n_heads=self.n_heads,
            n_encoder_layers=self.n_encoder_layers,
            use_entmax=True,
            entmax_n_iter=self.entmax_n_iter
        ).to(self.device)
        
        n_params = sum(p.numel() for p in self.model.parameters())
        print(f"  å‚æ•°é‡: {n_params:,}")
        
        return self.model
    
    def _load_best_checkpoint_for_stage(self, stage: str):
        """åŠ è½½å‰ä¸€é˜¶æ®µçš„æœ€ä½³checkpoint"""
        if stage == 'B':
            checkpoint_path = self.output_dir / 'checkpoint_stageA.pt'
            if checkpoint_path.exists():
                print(f"  åŠ è½½é˜¶æ®µA checkpoint: {checkpoint_path}")
                self.model.load_state_dict(safe_torch_load(checkpoint_path, map_location=self.device, weights_only=True))
        elif stage == 'C':
            checkpoint_path = self.output_dir / 'checkpoint_stageB.pt'
            if checkpoint_path.exists():
                print(f"  åŠ è½½é˜¶æ®µB checkpoint: {checkpoint_path}")
                self.model.load_state_dict(safe_torch_load(checkpoint_path, map_location=self.device, weights_only=True))
    
    def train_stage_A(self, train_loader, n_epochs=10, lr=1e-3, start_epoch=1):
        """è®­ç»ƒé˜¶æ®µA: Masked Reconstruction"""
        print("\n" + "="*80)
        print("é˜¶æ®µA: Masked Reconstruction é¢„è®­ç»ƒ")
        print("="*80 + "\n")
        
        trainer = MaskedReconstructionTrainer(self.model, self.device, use_amp=self.use_amp)
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)
        
        if start_epoch > 1:
            ckpt = self.checkpoint_mgr.load_last(self.model, optimizer,
                                                  trainer.scaler if self.use_amp else None,
                                                  self.device)
        
        for epoch in range(start_epoch, n_epochs + 1):
            loss = trainer.train_epoch(train_loader, optimizer, epoch)
            print(f"Epoch {epoch}/{n_epochs} - Loss: {loss:.4f}")
            
            self.logger.log('A', epoch, {'total': loss, 'rec': loss})
            
            self.checkpoint_mgr.save(
                self.model, optimizer, 'A', epoch, loss,
                {'n_epochs': n_epochs, 'lr': lr},
                trainer.scaler if self.use_amp else None
            )
        
        torch.save(self.model.state_dict(), 
                  self.output_dir / 'checkpoint_stageA.pt')
        print("\nâœ“ é˜¶æ®µAå®Œæˆ\n")
    
    def export_prototypes(self):
        """å¯¼å‡ºåŸå‹è§£é‡ŠåŒ…"""
        scaler_path = str(self.data_dir / 'scaler.pkl')
        metadata_path = str(self.data_dir / 'metadata.json')
        export_dir = str(self.output_dir / 'prototype_export')
        
        exporter = PrototypeExporter(
            self.model,
            scaler_path,
            metadata_path,
            export_dir,
            self.device
        )
        
        return exporter.export_all()
    
    def _export_gee_consistency_package(self):
        """å¯¼å‡ºGEEä¸€è‡´æ€§éªŒè¯åŒ…"""
        gee_dir = self.output_dir / 'gee_deploy'
        if not gee_dir.exists():
            print("  âš  GEEéƒ¨ç½²åŒ…ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸€è‡´æ€§åŒ…ç”Ÿæˆ")
            return
        
        # å¤åˆ¶å¿…è¦çš„éªŒè¯æ–‡ä»¶
        import shutil
        consistency_dir = self.output_dir / 'gee_consistency'
        consistency_dir.mkdir(exist_ok=True)
        
        # å¤åˆ¶é…ç½®æ–‡ä»¶
        for fname in ['distill_config.json', 'distill_weights.json']:
            src = gee_dir / fname
            if src.exists():
                shutil.copy(src, consistency_dir / fname)
        
        print(f"âœ“ GEEä¸€è‡´æ€§éªŒè¯åŒ…å·²å¯¼å‡º: {consistency_dir}")
    
    def train_stage_B(self, train_loader, n_epochs=20, lr=5e-4, start_epoch=1):
        """è®­ç»ƒé˜¶æ®µB: åŸå‹æ··åˆ (é›†æˆè¯Šæ–­)"""
        print("\n" + "="*80)
        print("é˜¶æ®µB: EntmaxåŸå‹æ··åˆ + ç”Ÿæˆå¼Decoder")
        print("="*80 + "\n")
        
        if start_epoch == 1:
            self._load_best_checkpoint_for_stage('B')
        
        trainer = PrototypeMixingTrainer(
            self.model, self.device, use_amp=self.use_amp
        )
        
        optimizer = torch.optim.AdamW([
            {'params': self.model.token_embedding.parameters(), 'lr': lr * 0.5},
            {'params': self.model.encoder.parameters(), 'lr': lr * 0.5},
            {'params': self.model.prototypes, 'lr': lr * 2.0},
            {'params': self.model.decoder.parameters(), 'lr': lr * 2.0},
            {'params': self.model.prototype_head.parameters(), 'lr': lr}
        ])
        
        if start_epoch > 1:
            ckpt = self.checkpoint_mgr.load_last(self.model, optimizer,
                                                  trainer.scaler if self.use_amp else None,
                                                  self.device)
        
        for epoch in range(start_epoch, n_epochs + 1):
            loss, loss_dict = trainer.train_epoch(train_loader, optimizer, epoch)
            print(f"Epoch {epoch}/{n_epochs} - "
                  f"Loss: {loss:.4f} | "
                  f"Rec: {loss_dict['rec']:.4f} | "
                  f"Balance: {loss_dict['balance']:.4f}")
            
            loss_dict['total'] = loss
            self.logger.log('B', epoch, loss_dict)
            
            self.checkpoint_mgr.save(
                self.model, optimizer, 'B', epoch, loss,
                {'n_epochs': n_epochs, 'lr': lr},
                trainer.scaler if self.use_amp else None
            )
            
            # ğŸ”¥ å…³é”®ï¼šæ¯éš” diagnostics_interval ä¸ªepochè¯Šæ–­ä¸€æ¬¡
            if self.diagnostics_enabled and epoch % self.diagnostics_interval == 0:
                print(f"\nâ†’ åœ¨Epoch {epoch}åè¿è¡Œè¯Šæ–­...")
                self._run_inline_diagnostics(train_loader, stage='B', epoch=epoch)
        
        torch.save(self.model.state_dict(), 
                  self.output_dir / 'checkpoint_stageB.pt')
        print("\nâœ“ é˜¶æ®µBå®Œæˆ\n")
    
    def train_stage_C(self, train_loader, n_epochs=10, lr=1e-4, start_epoch=1):
        """è®­ç»ƒé˜¶æ®µC: ç‰©ç†ç©ºé—´åˆ†ç¦» (é›†æˆè¯Šæ–­)"""
        print("\n" + "="*80)
        print("é˜¶æ®µC: ç‰©ç†ç©ºé—´åŸå‹åˆ†ç¦»")
        print("="*80 + "\n")
        
        if start_epoch == 1:
            self._load_best_checkpoint_for_stage('C')
        
        scaler_path = self.data_dir / 'scaler.pkl'
        trainer = PhysicalSeparationTrainer(
            self.model, self.device, str(scaler_path), use_amp=self.use_amp
        )
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)
        
        if start_epoch > 1:
            ckpt = self.checkpoint_mgr.load_last(self.model, optimizer,
                                                  trainer.scaler if self.use_amp else None,
                                                  self.device)
        
        for epoch in range(start_epoch, n_epochs + 1):
            loss, loss_dict = trainer.train_epoch(train_loader, optimizer, epoch)
            print(f"Epoch {epoch}/{n_epochs} - "
                  f"Loss: {loss:.4f} | "
                  f"Rec: {loss_dict['rec']:.4f} | "
                  f"Sep_Phys: {loss_dict['sep_phys']:.4f}")
            
            loss_dict['total'] = loss
            self.logger.log('C', epoch, loss_dict)
            
            self.checkpoint_mgr.save(
                self.model, optimizer, 'C', epoch, loss,
                {'n_epochs': n_epochs, 'lr': lr},
                trainer.scaler if self.use_amp else None
            )
            
            # ğŸ”¥ å…³é”®ï¼šæ¯éš” diagnostics_interval ä¸ªepochè¯Šæ–­ä¸€æ¬¡
            if self.diagnostics_enabled and epoch % self.diagnostics_interval == 0:
                print(f"\nâ†’ åœ¨Epoch {epoch}åè¿è¡Œè¯Šæ–­...")
                self._run_inline_diagnostics(train_loader, stage='C', epoch=epoch)
        
        torch.save(self.model.state_dict(), 
                  self.output_dir / 'checkpoint_stageC_final.pt')
        print("\nâœ“ é˜¶æ®µCå®Œæˆ\n")
    
    def _run_inline_diagnostics(self, dataloader, stage: str, epoch: int):
        """åœ¨çº¿è¯Šæ–­ (åœ¨è®­ç»ƒä¸­ç›´æ¥è¿è¡Œï¼Œæ¨¡å‹æ— éœ€é‡æ–°åŠ è½½)"""
        print("\n" + "="*80)
        print(f"ã€å†…è”è¯Šæ–­ã€‘Stage {stage} Epoch {epoch}")
        print("="*80)
        
        # åˆ›å»ºè¯Šæ–­å™¨ (ä½¿ç”¨é‡‡æ ·æ¨¡å¼ï¼Œæ¯æ¬¡æœ€å¤šå¤„ç†50Kæ ·æœ¬)
        proto_diag = PrototypeDiagnostics(
            self.model,
            self.device,
            str(self.output_dir),
            max_samples=50000  # 50Kæ ·æœ¬è¶³å¤Ÿç»Ÿè®¡ï¼Œé¿å…å…¨é‡7.7Méå†
        )
        
        # è¿è¡Œè¯Šæ–­ï¼ˆä½¿ç”¨ç°æœ‰çš„dataloaderï¼Œæ— éœ€é‡æ–°åŠ è½½æ¨¡å‹ï¼‰
        usage_stats = proto_diag.compute_prototype_usage(dataloader, save_plots=True)
        aez_stats = proto_diag.compute_aez_prototype_heatmap(dataloader, save_plots=True)
        
        # ä¿å­˜è¯Šæ–­ç»“æœåˆ°é˜¶æ®µ/epochç‰¹å®šçš„æ–‡ä»¶
        diag_record = {
            'stage': stage,
            'epoch': epoch,
            'timestamp': datetime.datetime.now().isoformat(),
            'n_dead_prototypes': len(usage_stats['dead_prototypes']),
            'dead_prototypes': usage_stats['dead_prototypes'],
            'mean_usage_rate': float(usage_stats['usage_rate'].mean()),
            'usage_rate_std': float(usage_stats['usage_rate'].std())
        }
        
        # è¿½åŠ åˆ°è¯Šæ–­æ—¥å¿—
        diag_log_file = self.output_dir / 'diagnostics' / f'stage{stage}_diagnostic_log.csv'
        diag_log_df = pd.DataFrame([diag_record])
        
        if diag_log_file.exists():
            existing = pd.read_csv(diag_log_file)
            diag_log_df = pd.concat([existing, diag_log_df], ignore_index=True)
        
        diag_log_df.to_csv(diag_log_file, index=False)
        
        print(f"\nâœ“ Stage {stage} Epoch {epoch} è¯Šæ–­å®Œæˆ")
        print(f"  Dead Prototypes: {len(usage_stats['dead_prototypes'])}")
        print(f"  å¹³å‡ä½¿ç”¨ç‡: {usage_stats['usage_rate'].mean()*100:.2f}%")
        print("="*80 + "\n")
        
        return {
            'usage_stats': usage_stats,
            'aez_stats': aez_stats
        }
    
    def distill_and_export(self, 
                          train_loader,
                          hidden_dims=[256, 128],
                          temperature=1.5,
                          n_epochs=20,
                          lr=1e-3,
                          max_distill_batches: Optional[int] = 1000,
                          save_distill_dataset=False):
        """è’¸é¦å­¦ç”Ÿæ¨¡å‹å¹¶å¯¼å‡º (é›†æˆè¯Šæ–­)"""
        print("\n" + "="*80)
        print("è’¸é¦è½»é‡MLPå­¦ç”Ÿæ¨¡å‹")
        print("="*80 + "\n")
        
        if save_distill_dataset:
            print(f"ç”Ÿæˆè’¸é¦æ•°æ®é›† (æœ€å¤š {max_distill_batches} æ‰¹æ¬¡)...")
            distill_dir = self.output_dir / 'distillation_dataset'
            distill_dir.mkdir(exist_ok=True)
            
            X_list = []
            w_teacher_list = []
            
            self.model.eval()
            with torch.no_grad():
                for batch_idx, batch in enumerate(tqdm(train_loader, desc="ç”Ÿæˆè’¸é¦æ•°æ®")):
                    if max_distill_batches is not None and batch_idx >= max_distill_batches:
                        break
                    
                    X = batch['X'].to(self.device)
                    if self.use_amp:
                        with autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):
                            outputs = self.model(X, stage='B')
                    else:
                        outputs = self.model(X, stage='B')
                    w_teacher = outputs['w']
                    
                    X_list.append(X.cpu().numpy())
                    w_teacher_list.append(w_teacher.cpu().numpy())
            
            X_distill = np.vstack(X_list)
            w_distill = np.vstack(w_teacher_list)
            
            del X_list, w_teacher_list
            gc.collect()
            
            np.savez_compressed(
                distill_dir / 'distill_data.npz',
                X=X_distill,
                w_teacher=w_distill
            )
            
            print(f"  âœ“ è’¸é¦æ•°æ®é›†å·²ä¿å­˜: {X_distill.shape[0]:,} æ ·æœ¬")
            
            del X_distill, w_distill
            gc.collect()
        
        self.student_model = DistillationMLPStudent(
            input_dim=121,
            hidden_dims=hidden_dims,
            output_dim=self.n_prototypes,
            activation='tanh',
            temperature=temperature
        ).to(self.device)
        
        n_params = sum(p.numel() for p in self.student_model.parameters())
        print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°é‡: {n_params:,}")
        
        trainer = DistillationTrainer(
            self.model, self.student_model, self.device, use_amp=self.use_amp
        )
        
        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=lr)
        
        for epoch in range(1, n_epochs + 1):
            loss, loss_components = trainer.train_epoch(
                train_loader, optimizer, epoch,
                max_batches=max_distill_batches
            )
            print(f"Epoch {epoch}/{n_epochs} - Loss: {loss:.4f} | "
                  f"KL: {loss_components['kl']:.4f} | Entropy: {loss_components['entropy']:.4f}")
            self.logger.log('Distill', epoch, {
                'total': loss, 
                'kl': loss_components['kl'],
                'entropy': loss_components['entropy']
            })
            
            # ğŸ”¥ å…³é”®ï¼šè’¸é¦å®Œæˆåç«‹å³è¯Šæ–­ä¸€è‡´æ€§
            if epoch == n_epochs:
                print(f"\nâ†’ è’¸é¦å®Œæˆï¼Œè¿è¡ŒTeacher-Studentä¸€è‡´æ€§è¯Šæ–­...")
                self._run_inline_distillation_diagnostics(train_loader)
        
        torch.save(self.student_model.state_dict(),
                  self.output_dir / 'distill_student.pt')
        
        exporter = DistillationExporter(
            self.student_model,
            str(self.output_dir / 'gee_deploy'),
            self.metadata
        )
        
        results = exporter.export_all()
        print("\nâœ“ è’¸é¦å®Œæˆ\n")
        
        return results
    
    def _run_inline_distillation_diagnostics(self, dataloader):
        """è’¸é¦è¯Šæ–­ (åœ¨çº¿ç‰ˆæœ¬)"""
        print("\n" + "="*80)
        print("ã€å†…è”è¯Šæ–­ã€‘è’¸é¦ä¸€è‡´æ€§åˆ†æ")
        print("="*80)
        
        distill_diag = DistillationDiagnostics(
            self.model,
            self.student_model,
            self.device,
            str(self.output_dir)
        )
        
        distill_stats = distill_diag.compute_distillation_consistency(
            dataloader,
            save_plots=True
        )
        
        print("="*80 + "\n")
        
        return distill_stats
    
    def run_full_pipeline(self,
                         batch_size=512,
                         n_batches_per_epoch_A=3000,
                         n_batches_per_epoch_B=3000,
                         n_batches_per_epoch_C=1000,
                         stage_A_epochs=5,
                         stage_B_epochs=15,
                         stage_C_epochs=8,
                         distill_epochs=15,
                         max_distill_batches=1000,
                         auto_resume=True,
                         enable_diagnostics=True,
                         diagnostics_interval=5):
        """
        è¿è¡Œå®Œæ•´è®­ç»ƒç®¡é“ (é›†æˆè¯Šæ–­)
        
        å‚æ•°:
            enable_diagnostics: æ˜¯å¦å¯ç”¨åœ¨çº¿è¯Šæ–­
            diagnostics_interval: æ¯Nä¸ªepochè¯Šæ–­ä¸€æ¬¡
        """
        print("\n" + "="*80)
        print("DeepAA å®Œæ•´è®­ç»ƒç®¡é“ (é›†æˆè¯Šæ–­)")
        print("="*80)
        
        self.diagnostics_enabled = enable_diagnostics
        self.diagnostics_interval = diagnostics_interval
        
        if enable_diagnostics:
            print(f"âœ“ åœ¨çº¿è¯Šæ–­å·²å¯ç”¨ (æ¯{diagnostics_interval}ä¸ªepochè¯Šæ–­ä¸€æ¬¡)")
        else:
            print("âš  åœ¨çº¿è¯Šæ–­å·²ç¦ç”¨")
        
        # ... [åŸæœ‰çš„æ¢å¤é€»è¾‘å’Œé˜¶æ®µè®­ç»ƒä»£ç ] ...
        
        resume_info = None
        if auto_resume:
            resume_info = self.checkpoint_mgr.get_resume_info()
            if resume_info:
                print(f"\nâš¡ æ£€æµ‹åˆ°æ–­ç‚¹:")
                print(f"   Stage: {resume_info['stage']}")
                print(f"   Epoch: {resume_info['epoch']}")
                print(f"   æ—¶é—´: {resume_info['timestamp']}")
        
        self.build_model()
        
        start_stage = 'A'
        start_epoch_A = 1
        start_epoch_B = 1
        start_epoch_C = 1
        
        if resume_info:
            if resume_info['stage'] == 'A':
                start_stage = 'A'
                start_epoch_A = resume_info['epoch'] + 1
                if start_epoch_A > stage_A_epochs:
                    start_stage = 'B'
                    start_epoch_A = stage_A_epochs + 1
            elif resume_info['stage'] == 'B':
                start_stage = 'B'
                start_epoch_B = resume_info['epoch'] + 1
                if start_epoch_B > stage_B_epochs:
                    start_stage = 'C'
                    start_epoch_B = stage_B_epochs + 1
            elif resume_info['stage'] == 'C':
                start_stage = 'C'
                start_epoch_C = resume_info['epoch'] + 1
                if start_epoch_C > stage_C_epochs:
                    start_stage = 'DONE'
            
            if start_stage == resume_info['stage']:
                self.checkpoint_mgr.load_last(self.model, device=self.device)
        
        # é˜¶æ®µA
        if start_stage == 'A':
            train_loader_A = self.load_data(
                batch_size=batch_size,
                use_stratified=True,
                num_workers=2,
                n_batches_per_epoch=n_batches_per_epoch_A
            )
            self.train_stage_A(train_loader_A, n_epochs=stage_A_epochs, 
                              start_epoch=start_epoch_A)
            start_stage = 'B'
            start_epoch_B = 1
            del train_loader_A
            gc.collect()
        
        # é˜¶æ®µB
        if start_stage == 'B':
            train_loader_B = self.load_data(
                batch_size=batch_size,
                use_stratified=True,
                num_workers=2,
                n_batches_per_epoch=n_batches_per_epoch_B
            )
            self.train_stage_B(train_loader_B, n_epochs=stage_B_epochs,
                              start_epoch=start_epoch_B)
            start_stage = 'C'
            start_epoch_C = 1
            del train_loader_B
            gc.collect()
        
        # é˜¶æ®µC
        if start_stage == 'C':
            train_loader_C = self.load_data(
                batch_size=batch_size,
                use_stratified=True,
                num_workers=2,
                n_batches_per_epoch=n_batches_per_epoch_C
            )
            self.train_stage_C(train_loader_C, n_epochs=stage_C_epochs,
                              start_epoch=start_epoch_C)
            del train_loader_C
            gc.collect()
        
        # å¯¼å‡ºåŸå‹
        print("\nå¼€å§‹å¯¼å‡º...")
        proto_results = self.export_prototypes()
        
        # è’¸é¦å¹¶å¯¼å‡º
        train_loader_distill = self.load_data(
            batch_size=batch_size,
            use_stratified=True,
            num_workers=2,
            n_batches_per_epoch=max_distill_batches
        )
        
        distill_results = self.distill_and_export(
            train_loader_distill,
            n_epochs=distill_epochs,
            max_distill_batches=max_distill_batches,
            save_distill_dataset=False
        )
        
        self._export_gee_consistency_package()
        
        self.logger.plot_training_curves()
        
        print("\n" + "="*80)
        print("âœ“âœ“âœ“ å®Œæ•´è®­ç»ƒç®¡é“å®Œæˆï¼è¯Šæ–­ç»“æœå·²åœ¨çº¿ç”Ÿæˆ âœ“âœ“âœ“")
        print("="*80)
        print("\nç”Ÿæˆçš„æ–‡ä»¶ä½ç½®:")
        print(f"  åŸå‹è§£é‡ŠåŒ…: {self.output_dir / 'prototype_export'}")
        print(f"  GEEéƒ¨ç½²åŒ…: {self.output_dir / 'gee_deploy'}")
        print(f"  è¯Šæ–­ç»“æœ: {self.output_dir / 'diagnostics'}")
        print(f"  è®­ç»ƒæ—¥å¿—: {self.output_dir / 'train_log.csv'}")
        print(f"  è®­ç»ƒæ›²çº¿: {self.output_dir / 'training_curves.png'}")
        print("\nè¯Šæ–­æ–‡ä»¶:")
        print(f"  âœ“ prototype_usage_histogram.png")
        print(f"  âœ“ prototype_usage_stats.csv")
        print(f"  âœ“ aez_prototype_heatmap.png")
        print(f"  âœ“ aez_prototype_heatmap.csv")
        print(f"  âœ“ distillation_consistency_distributions.png")
        print(f"  âœ“ distillation_consistency_summary.csv")
        print(f"  âœ“ weight_scatter.png")
        print("="*80 + "\n")
        
        return {
            'prototype_export': proto_results,
            'distill_export': distill_results
        }
    

# @title
# ============================================================================
# 9. ä¸»ç¨‹åºå…¥å£ (Colabä¸“ç”¨)
# ============================================================================

def main_colab():
    """Colabä¸»å‡½æ•°"""
    
    # ==================== 1. ç¯å¢ƒè®¾ç½® ====================
    IN_COLAB = setup_colab_env()
    
    # ==================== 2. è·¯å¾„é…ç½® ====================
    
    # Driveä¸Šçš„æ•°æ®æºè·¯å¾„ (è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹)
    DRIVE_DATA_DIR = "/content/drive/MyDrive/DeepAA_TokenData"
    
    # æœ¬åœ°æ•°æ®è·¯å¾„ (è®­ç»ƒæ—¶ä»è¿™é‡Œè¯»å–ï¼Œé€Ÿåº¦å¿«)
    LOCAL_DATA_DIR = "/content/DeepAA_TokenData"
    
    # è¾“å‡ºè·¯å¾„ (ç›´æ¥å†™åˆ°Driveï¼Œé˜²æ­¢æ–­è¿ä¸¢å¤±)
    OUTPUT_DIR = "/content/drive/MyDrive/DeepAA/DeepAA_Training_Run1"
    
    # ==================== 3. æ•°æ®å‡†å¤‡ ====================
    
    # å°†Driveæ•°æ®å¤åˆ¶åˆ°æœ¬åœ°
    if IN_COLAB:
        DATA_DIR = copy_data_to_local(DRIVE_DATA_DIR, LOCAL_DATA_DIR)
    else:
        # æœ¬åœ°æµ‹è¯•æ—¶ç›´æ¥ç”¨æºè·¯å¾„
        DATA_DIR = DRIVE_DATA_DIR
    
    # ==================== 4. è®­ç»ƒå‚æ•°é…ç½® ====================
    
    # ğŸš€ æ‰¹æ¬¡å¤§å° 
    BATCH_SIZE = 4096
    
    # ğŸš€ æ¯epochæ­¥æ•° (å…³é”®ï¼šæ§åˆ¶è®­ç»ƒæ—¶é—´)
    # 773ä¸‡æ ·æœ¬å…¨è·‘ä¸€éçº¦15000æ­¥ï¼Œè¿™é‡Œåªè·‘å­é›†
    N_BATCHES_PER_EPOCH_A = 800    # é˜¶æ®µA: ~300ä¸‡æ ·æœ¬/epoch
    N_BATCHES_PER_EPOCH_B = 800    # é˜¶æ®µB: ~300ä¸‡æ ·æœ¬/epoch
    N_BATCHES_PER_EPOCH_C = 300    # é˜¶æ®µC: ~100ä¸‡æ ·æœ¬/epoch
    
    # ğŸš€ è®­ç»ƒè½®æ•°
    STAGE_A_EPOCHS = 5    # é¢„è®­ç»ƒ
    STAGE_B_EPOCHS = 10   # åŸå‹æ··åˆ
    STAGE_C_EPOCHS = 5    # ç‰©ç†åˆ†ç¦»
    DISTILL_EPOCHS = 10   # è’¸é¦
    
    # ğŸš€ è’¸é¦å­é›† (ä¸éœ€è¦å…¨é‡æ•°æ®)
    MAX_DISTILL_BATCHES = 100  # ~50ä¸‡æ ·æœ¬
    
    # ğŸš€ æ¨¡å‹ç»“æ„ (å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦)
    D_MODEL = 256           # 128å¿«/256å¹³è¡¡/512æ…¢
    N_HEADS = 8             # æ³¨æ„åŠ›å¤´æ•°
    N_ENCODER_LAYERS = 4    # Encoderå±‚æ•°
    ENTMAX_N_ITER = 15      # Entmaxè¿­ä»£æ¬¡æ•° (15å¤Ÿç”¨ï¼Œ50å¤ªæ…¢)
    
    # åŸå‹æ•°é‡
    N_PROTOTYPES = 60
    
    # æ•°æ®åŠ è½½
    NUM_WORKERS = 4         # Colabå»ºè®®2-4
    
    # AMPæ··åˆç²¾åº¦
    USE_AMP = True
    
    # éšæœºç§å­
    SEED = 42
    
    # ==================== 5. è¿è¡Œè®­ç»ƒ ====================
    
    print("\n" + "="*80)
    print("âš¡ Colabè®­ç»ƒé…ç½®")
    print("="*80)
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"æ¯epochæ­¥æ•°: A={N_BATCHES_PER_EPOCH_A}, B={N_BATCHES_PER_EPOCH_B}, C={N_BATCHES_PER_EPOCH_C}")
    print(f"è®­ç»ƒè½®æ•°: A={STAGE_A_EPOCHS}, B={STAGE_B_EPOCHS}, C={STAGE_C_EPOCHS}, Distill={DISTILL_EPOCHS}")
    print(f"æ¨¡å‹: d={D_MODEL}, heads={N_HEADS}, layers={N_ENCODER_LAYERS}")
    print(f"åŸå‹æ•°: {N_PROTOTYPES}")
    print(f"AMP: {USE_AMP}")
    print("="*80 + "\n")
    
    # åˆ›å»ºç®¡é“
    pipeline = DeepAATrainingPipelineColab(
        data_dir=DATA_DIR,
        output_dir=OUTPUT_DIR,
        n_prototypes=N_PROTOTYPES,
        d_model=D_MODEL,
        n_heads=N_HEADS,
        n_encoder_layers=N_ENCODER_LAYERS,
        entmax_n_iter=ENTMAX_N_ITER,
        use_amp=USE_AMP,
        seed=SEED
    )
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    results = pipeline.run_full_pipeline(
        batch_size=BATCH_SIZE,
        n_batches_per_epoch_A=N_BATCHES_PER_EPOCH_A,
        n_batches_per_epoch_B=N_BATCHES_PER_EPOCH_B,
        n_batches_per_epoch_C=N_BATCHES_PER_EPOCH_C,
        stage_A_epochs=STAGE_A_EPOCHS,
        stage_B_epochs=STAGE_B_EPOCHS,
        stage_C_epochs=STAGE_C_EPOCHS,
        distill_epochs=DISTILL_EPOCHS,
        max_distill_batches=MAX_DISTILL_BATCHES,
        auto_resume=True,
        enable_diagnostics=True,    
        diagnostics_interval=5    
    )
    return results


# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼ˆåœ¨Colabä¸­ï¼‰
if __name__ == '__main__':
    results = main_colab()